{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from requests.structures import CaseInsensitiveDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from joblib import load\n",
    "from api_keys import geolocation_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages = [\n",
    "    \"Python\", \"JavaScript\", \"Java\", \"C#\", \"C++\", \"PHP\", \"Ruby\", \"Swift\",\n",
    "    \"Golang\", \"TypeScript\", \"R\", \"Kotlin\", \"HTML\", \"CSS\", \"SQL\", \"Rust\", \"Dart\",\n",
    "    \"Scala\", \"Perl\", \"Haskell\", \"Elixir\", \"Lua\", \"Bash\",\n",
    "    \"Objective-C\", \"Groovy\", \"F#\", \"Visual Basic\", \"MATLAB\", \"Assembly Language\",\n",
    "    \"Julia\", \"Cobol\", \"Fortran\", \"Erlang\", \"Prolog\", \"Crystal\", \"Tcl\", \"Lisp\",\n",
    "    \"ActionScript\", \"Apex\", \"Clojure\", \"Smalltalk\", \"Solidity\", \"NIM\", \n",
    "    \"OCaml\", \"Q#\", \"Ceylon\", \"VBScript\", \"Awk\", \"Racket\",\n",
    "    \"ABAP\", \"XSLT\", \"Lasso\", \"Max\", \"PostScript\", \n",
    "    \"Simulink\", \"Io\", \"Datalog\", \"Pliant\", \"J\", \"GAMS\", \"Gherkin\", \"SuperCollider\",\n",
    "    \"Sed\", \"PASCAL\"\n",
    "]\n",
    "\n",
    "for i in range(len(programming_languages)) :\n",
    "    if \".\" in programming_languages[i] :\n",
    "        programming_languages.append(programming_languages[i].replace(\".\",\"\"))\n",
    "\n",
    "databases = [\n",
    "    \"MySQL\",\n",
    "    \"PostgreSQL\",\n",
    "    \"SQLite\",\n",
    "    \"MongoDB\",\n",
    "    \"SQL Server\",\n",
    "    \"Oracle\",\n",
    "    \"Redis\",\n",
    "    \"Cassandra\",\n",
    "    \"MariaDB\",\n",
    "    \"DB2\",\n",
    "    \"Amazon DynamoDB\",\n",
    "    \"Firebase Realtime\",\n",
    "    \"Elasticsearch\",\n",
    "    \"Azure SQL\",\n",
    "    \"SAP HANA\",\n",
    "    \"CockroachDB\",\n",
    "    \"Google Cloud Firestore\",\n",
    "    \"Couchbase\",\n",
    "    \"Neo4j\",\n",
    "    \"HBase\",\n",
    "    \"Apache Hive\",\n",
    "    \"Teradata\",\n",
    "    \"Apache Cassandra\",\n",
    "    \"ArangoDB\",\n",
    "    \"RavenDB\",\n",
    "    \"Citus\",\n",
    "    \"TimescaleDB\",\n",
    "    \"OrientDB\",\n",
    "    \"Titan\",\n",
    "    \"TokuMX\",\n",
    "    \"VoltDB\",\n",
    "    \"Memcached\",\n",
    "    \"OpenTSDB\",\n",
    "    \"Dgraph\",\n",
    "    \"MarkLogic\",\n",
    "    \"Apache Drill\",\n",
    "    \"CouchDB\",\n",
    "    \"SAP IQ\",\n",
    "    \"NuoDB\",\n",
    "    \"Amazon Aurora\",\n",
    "    \"Azure Cosmos DB\",\n",
    "    \"CrateDB\",\n",
    "    \"Greenplum\",\n",
    "    \"Pivotal GemFire\",\n",
    "    \"EventStore\",\n",
    "    \"SQL Anywhere\",\n",
    "    \"DataStax Enterprise\",\n",
    "    \"AllegroGraph\",\n",
    "    \"Presto\",\n",
    "    \"Amazon Redshift\",\n",
    "    \"Informix\",\n",
    "    \"Apache Kudu\",\n",
    "    \"Sybase\",\n",
    "    \"Firebird\",\n",
    "    \"Apache Derby\",\n",
    "    \"SQLite\",\n",
    "    \"MaxDB\",\n",
    "    \"Teradata Aster\",\n",
    "    \"Vertica\",\n",
    "    \"Linterra\",\n",
    "    \"RocksDB\",\n",
    "    \"ClickHouse\",\n",
    "    \"Hadoop HDFS\",\n",
    "    \"Joomla\",\n",
    "    \"MSSQL\",\n",
    "    \"Xbase\",\n",
    "    \"Zebra\",\n",
    "    \"Wikidata\",\n",
    "    \"QlikView\",\n",
    "    \"Druid\",\n",
    "    \"Apache Phoenix\",\n",
    "    \"HSQLDB\",\n",
    "    \"Realm\",\n",
    "    \"CockroachDB\",\n",
    "    \"QLDB\",\n",
    "    \"Tarantool\",\n",
    "    \"Couchbase\",\n",
    "    \"Sphinx\",\n",
    "    \"InterBase\",\n",
    "    \"PouchDB\",\n",
    "    \"RavenDB\",\n",
    "    \"Pivotal Greenplum\",\n",
    "    \"TQL\",\n",
    "    \"Blazegraph\",\n",
    "    \"Netezza\",\n",
    "    \"Exasol\",\n",
    "    \"Coda\",\n",
    "    \"Qlik Sense\",\n",
    "    \"Linterra\",\n",
    "    \"Glean\",\n",
    "    \"OpenCensus\",\n",
    "    \"Snowflake\",\n",
    "    \"Metabase\"\n",
    "]\n",
    "for i in range(len(databases)) :\n",
    "    if \".\" in databases[i] :\n",
    "        print(databases[i])\n",
    "        databases.append(databases[i].replace(\".\",\"\"))\n",
    "\n",
    "frameworks = [\n",
    "    # Web Development\n",
    "    \"Kafka\",\n",
    "    \"SwiftUI\",\"Node.js\",\"React\", \"Angular\", \"Vue.js\", \"Django\", \"Flask\", \"Ruby on Rails\", \"Express.js\", \n",
    "    \"ASP.NET\", \"Spring\", \"Laravel\", \"Symfony\", \"FastAPI\", \"Svelte\", \"Backbone.js\", \n",
    "    \"CodeIgniter\", \"NestJS\", \"Meteor\", \"Pyramid\", \"Phoenix\", \n",
    "    \"Ionic\", \"Bootstrap\", \"Bulma\", \"Materialize\", \"Tailwind CSS\", \n",
    "    \"Ember.js\", \"Next.js\", \"Nuxt.js\", \"Gatsby\", \"Zope\", \"Sinatra\", \"JSP\", \n",
    "    \"Play Framework\", \"Tornado\", \"Web2py\", \"Sequelize\", \"Knex.js\", \"Deno\", \n",
    "    \"Sanity\", \"Strapi\", \"GraphQL\", \"RESTful API\", \"Jekyll\", \"Hugo\", \"Docusaurus\", \n",
    "    \"Aurelia\", \"Mithril\", \"Quasar\", \"Alpine.js\", \"Elm\", \"ClojureScript\", \n",
    "    \"PicoCMS\", \"Ant Design\", \"PrimeNG\", \"Semantic UI\", \"jQuery\", \n",
    "    \"Preact\", \"Turbo\", \"AppSync\", \"Flask-SocketIO\", \"Vapor\", \n",
    "    \"Jersey\", \"Vaadin\", \"Pico\", \"YII\", \"Tiki Wiki\", \"Phalcon\", \"Hapi.js\", \n",
    "    \"Koa.js\", \"Restify\", \"Slim\", \"Silex\", \"Liquid\", \"BootstrapVue\", \"Fomantic UI\", \n",
    "    \"Aurelia\", \"Chai\", \"Sass\", \"PostCSS\", \"jQuery UI\", \"Kendo UI\", \n",
    "    \"Dojo\", \"Webix\", \"Gijgo\", \"Pikaday\", \"FullCalendar\", \"Handsontable\", \n",
    "    \"GrapeJS\", \"Frappe\", \"Phabricator\", \"CakePHP\", \"Nette\", \"Yii2\", \"SilverStripe\", \n",
    "    \"OroCRM\", \"TYPO3\", \"CouchCMS\", \"Concrete5\", \"Grav\", \"Kirby\", \"OctoberCMS\", \n",
    "    \"ProcessWire\", \"MODX\", \"Craft CMS\", \"Pimcore\", \"Bolt\", \"Umbraco\", \"DotNetNuke\",\n",
    "\n",
    "    # Mobile Development\n",
    "    \"React Native\", \"Flutter\", \"Xamarin\", \"Ionic\", \"Apache Cordova\", \"USwiftI\", \n",
    "    \"Kotlin Multiplatform Mobile\", \"PhoneGap\", \"NativeScript\", \"Sencha Touch\", \n",
    "    \"Appcelerator Titanium\", \"Framework7\", \"Unity\", \"Cocos2d-x\", \"Fusetools\", \n",
    "    \"NativeBase\", \"Quasar Framework\", \"Onsen UI\", \"Cordova\", \"ReactXP\", \n",
    "    \"Tauri\", \"Kivy\", \"PyQt5\", \"wxPython\", \"BeeWare\", \"Gluon\", \"Crosswalk\", \n",
    "    \"Fyn\", \"Material Components for Android\", \"Robolectric\", \"Apache Felix\", \n",
    "    \"Zygote\", \"UI Automator\", \"MonkeyRunner\", \"Firebase UI\", \"Codename One\", \n",
    "    \"Nativescript-Vue\", \"RxJava\", \"Dagger\", \"ButterKnife\", \"Retrofit\", \"Volley\", \n",
    "    \"OkHttp\", \"RxAndroid\", \"Firebase Cloud Messaging\", \"Fastlane\", \"Swift Package Manager\",\n",
    "\n",
    "    # Game Development\n",
    "    \"Unity\", \"Unreal Engine\", \"Godot\", \"Cocos2d\", \"CryEngine\", \"GameMaker Studio\", \n",
    "    \"Phaser\", \"LibGDX\", \"Defold\", \"Ren'Py\", \"SpriteKit\", \"Cocos2d-x\", \n",
    "    \"Pygame\", \"MonoGame\", \"JMonkeyEngine\", \"GameSalad\", \"Havok\", \"Fmod\", \"Unity3D\", \n",
    "    \"CryEngine\", \"Torque3D\", \"PlayCanvas\", \"Blend4Web\", \"Panda3D\", \"CopperLicht\", \n",
    "    \"Ogre3D\", \"Three.js\", \"Babylon.js\", \"Fusio\", \"LÖVE\", \"BGE\", \"Cinder\", \n",
    "    \"Game Framework\", \"Wave Engine\", \"PICO-8\", \"Phaser Editor\", \"GDevelop\", \n",
    "    \"Cocos Creator\", \"Chocolat\", \"Bevy\", \"Flixel\", \"Luxe\", \"Leadwerks\", \n",
    "    \"Allegro\", \"GML\", \"Squirrel\", \"Visual3D\", \"Sunburn\", \"C4\", \"Zenject\",\n",
    "\n",
    "    # Data Science and Machine Learning\n",
    "    \"PySpark\",\"TensorFlow\", \"Keras\", \"PyTorch\", \"Scikit\", \"Pandas\", \"NumPy\", \n",
    "    \"Matplotlib\", \"Seaborn\", \"Spark\", \"Dask\", \"H2O.ai\", \"Apache Flink\", \n",
    "    \"Hadoop\", \"OpenCV\", \"NLTK\", \"spaCy\", \"FastAI\", \"XGBoost\", \"LightGBM\", \n",
    "    \"CatBoost\", \"MLflow\", \"Airflow\", \"PyCaret\", \"Dask-ML\", \"Haystack\", \n",
    "    \"Shiny\", \"Dash\", \"Plotly\", \"Streamlit\", \"Bokeh\", \"TensorBoard\", \n",
    "    \"Chainer\", \"Pytorch Lightning\", \"TPOT\", \"DataRobot\", \"AutoML\", \"TPOT\", \n",
    "    \"Optuna\", \"Ray\", \"FastText\", \"gensim\", \"Featuretools\", \"Weka\", \n",
    "    \"KNIME\", \"Alteryx\", \"Datarobot\", \"Metaflow\", \"Jupyter Notebook\", \n",
    "     \"Dash\", \"Orange3\", \"DeepSpeed\", \"Fairlearn\", \"FiftyOne\",\n",
    "\n",
    "    # DevOps and CI/CD\n",
    "     \"Babel\",\"Vite\",\"Parcel\",\"Webpack\",\"Grunt\",\n",
    "\n",
    "    # Testing Frameworks\n",
    "    \"JUnit\", \"pytest\", \"Selenium\", \"Mocha\", \"Chai\", \"Jasmine\", \"Cypress\", \n",
    "    \"TestNG\", \"RSpec\", \"Jest\", \"Karma\", \"Puppeteer\", \"Robot Framework\", \n",
    "    \"Protractor\", \"Postman\", \"Cucumber\", \"JUnit\", \"NUnit\", \"Vitest\", \n",
    "    \"Enzyme\", \"Cypress\", \"Supertest\", \"Mocha\", \"Mochawesome\", \"Pytest-bdd\", \n",
    "    \"Gatling\", \"Locust\", \"Trestle\", \"Avro\", \"WireMock\", \"Karate\", \n",
    "    \"Selenium Grid\", \"Applitools\", \"Gauge\", \"Cypress\", \"RestAssured\", \n",
    "    \"Flask-Testing\", \"Flask-RESTPlus\", \"Playwright\", \"SpecFlow\", \"Codacy\", \n",
    "    \"SonarQube\", \"Selenium IDE\",\n",
    "\n",
    "    # Desktop Applications\n",
    "    \"Electron\", \"Qt\", \"GTK\", \"JavaFX\", \"wxWidgets\", \"Avalonia\", \"Flutter Desktop\", \n",
    "    \"Nw.js\", \"Xamarin.Forms\", \"React Native Windows\", \"Tauri\", \"Electron Forge\", \n",
    "    \"PyQt\", \"Tkinter\", \"Kivy\", \"PySide\", \"WinForms\", \"MFC\", \"GTK\", \"UWP\", \n",
    "    \"Avalonia UI\", \"JavaFX\", \"Flutter\", \"Blazor\", \"Uno Platform\", \"WPF\", \n",
    "    \"Chocolat\", \"AppKit\", \"Mac Catalyst\", \"PyQt5\", \"Gnome\", \"Electron React\", \n",
    "    \"Cocoa\", \"Xamarin\", \"Delphi\", \"Qt Creator\", \"QT Quick\", \"Swing\", \n",
    "    \"Java Swing\", \"Pygame\", \"Avalonia\", \"VCL\",\n",
    "\n",
    "    # Other Frameworks and Libraries\n",
    "\n",
    "    \"FAIR\",\"KOBIT\", \"RabbitMQ\", \"Celery\", \"OpenShift\", \"Nginx\", \"Apache HTTP Server\" , \"SendGrid\", \"Twilio\", \"Stripe\", \"Socket.io\", \n",
    "    \"GraphQL\", \"gRPC\", \"Pusher\", \"Flask-RESTful\", \n",
    "    \"Jupyter Notebook\", \"Apache Thrift\", \"Hapi.js\", \"React Query\", \"Apollo Client\", \n",
    "    \"WebAssembly\", \"RxJS\", \"Flux\", \"MobX\", \"Gulp\", \"Webpack\", \"Parcel\", \n",
    "    \"Babel\", \"Grunt\", \"Vite\", \"Puppeteer\", \"Sass\", \"PostCSS\", \"Frappe\", \"Django REST Framework\", \"Netty\", \n",
    "    \"Spring Boot\", \"Play Framework\", \"Dropwizard\", \"Java EE\", \"Vaadin\", \n",
    "    \"Grails\", \"Avert\", \"Flyway\", \"JOOQ\", \"Spring Data\", \"MicroProfile\", \n",
    "    \"Ktor\", \"Vert.x\", \"Gson\", \"Jackson\", \"jOOQ\", \"Apache Ant\", \"Jenkins\", \n",
    "    \"Concourse\", \"Mercurial\", \"Bazaar\", \n",
    "    \"TFS\", \"Subversion\", \"Plastic SCM\", \"FogBugz\", \"Phabricator\", \"Helix Core\", \n",
    "    \"Gitea\", \"Gogs\", \"SourceGear Vault\", \"SmartSVN\", \"TortoiseSVN\", \n",
    "    \"Fossil\", \"SVN\", \"Aegis\",\n",
    "\n",
    "    # Additional Frameworks\n",
    "    \"ISO 27001\",\n",
    "\"MITRE ATT\",\n",
    "\"COBIT\",\n",
    "\"PCI DSS\",\n",
    "\"SOC 2\",\n",
    "\"GDPR\",\n",
    "\"ITIL\",\n",
    "\"CMMI\",\n",
    "\"Cis Controls\",\n",
    "    \"Spring Cloud\", \"Spring Security\", \"Spring MVC\", \"Spring Integration\", \n",
    "    \"Spring Batch\", \"Apache Camel\", \"Apache Shiro\", \"Canoe\", \"Libuv\", \n",
    "    \"ASP.NET MVC\", \"ASP.NET Core\", \"Apache Cordova\", \"Xamarin\", \"OSGi\", \n",
    "    \"React Query\", \"SWR\", \"Apollo Client\", \"Puppeteer\", \"Cheerio\", \n",
    "    \"Bootstrap\", \"Fomantic UI\", \"Materialize\", \"PrimeReact\", \n",
    "    \"Ant Design\", \"Semantic UI\", \"Spectre.css\", \"UIKit\", \"Material Design Lite\", \n",
    "    \"Milligram\", \"Skeleton\", \"Blaze UI\", \"Fomantic UI\", \"Semantic UI React\", \n",
    "    \"Stylus\", \"Foundation Sites\", \"HTML5 Boilerplate\", \"Normalize.css\", \n",
    "    \"PostCSS\", \"CSS Modules\", \"CSS-in-JS\", \"Styled Components\", \n",
    "    \"Emotion\", \"JSS\", \"Radium\", \"Shadow DOM\", \"LitElement\", \"HyperHTML\", \n",
    "    \"html.js\", \"Gatsby\", \"Next.js\", \"Nuxt.js\", \"Hugo\", \"Jekyll\", \"Docusaurus\", \n",
    "    \"Scully\", \"Sapper\", \"Gatsby\", \"Middleman\", \"Grunt\", \"Gulp\", \"npm\", \n",
    "    \"Yarn\", \"Webpack\", \"Parcel\", \"Vite\", \"Fly\", \"Zola\", \"Ziggy\", \"TiddlyWiki\", \n",
    "    \"Sphinx\", \"MkDocs\", \"Doxygen\", \"HDoc\", \"Doxygen\", \n",
    "    \"Javadoc\", \"Sphinx\", \"Hugo\", \"Jekyll\", \"MkDocs\"\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(len(frameworks)) :\n",
    "    if \".\" in frameworks[i] :\n",
    "        frameworks.append(frameworks[i].replace(\".\",\"\"))\n",
    "\n",
    "\n",
    "cloud_service_providers = [\n",
    "    \"Bigquery\",\n",
    "    \"Redshift\",\n",
    "    \"Databricks\",\n",
    "    \"AWS\",\n",
    "    \"Azure\",\n",
    "    \"GCP\",\n",
    "    \"IBM Cloud\",\n",
    "    \"Oracle Cloud\",\n",
    "    \"Salesforce\",\n",
    "    \"DigitalOcean\",\n",
    "    \"Linode\",\n",
    "    \"Vultr\",\n",
    "    \"Heroku\",\n",
    "    \"Rackspace\",\n",
    "    \"Cloudflare\",\n",
    "    \"Red Hat OpenShift\",\n",
    "    \"SAP Cloud Platform\",\n",
    "    \"Alibaba Cloud\",\n",
    "    \"Mendix\",\n",
    "    \"Cisco Cloud\",\n",
    "    \"Cloudways\",\n",
    "    \"Trello\",\n",
    "    \"Zoho\",\n",
    "    \"Atlassian Cloud\",\n",
    "    \"Smartsheet\",\n",
    "    \"Firebase\",\n",
    "    \"Contentful\",\n",
    "    \"Shopify\",\n",
    "    \"Zendesk\",\n",
    "    \"Fastly\",\n",
    "    \"Akamai\",\n",
    "    \"SiteGround\",\n",
    "    \"InMotion Hosting\",\n",
    "    \"WP Engine\",\n",
    "    \"GreenGeeks\",\n",
    "    \"A2 Hosting\",\n",
    "    \"HostGator\",\n",
    "    \"iPage\",\n",
    "    \"Bluehost\",\n",
    "    \"Liquid Web\",\n",
    "    \"DreamHost\",\n",
    "    \"Kinsta\",\n",
    "    \"CloudSigma\",\n",
    "    \"OVHcloud\",\n",
    "    \"CenturyLink Cloud\",\n",
    "    \"Gandi\",\n",
    "    \"Google Workspace\",\n",
    "    \"Microsoft 365\",\n",
    "    \"MaxCompute\",\n",
    "    \"Clever Cloud\",\n",
    "    \"Render\",\n",
    "    \"Platform.sh\",\n",
    "    \"Back4App\",\n",
    "    \"Kinvey\",\n",
    "    \"GCP Firebase\",\n",
    "    \"Bitbucket\",\n",
    "    \"Vercel\",\n",
    "    \"Netlify\",\n",
    "    \"Glitch\",\n",
    "    \"Heroku Postgres\",\n",
    "    \"Linode Block Storage\",\n",
    "    \"Cloudian\",\n",
    "    \"Pivotal Cloud Foundry\",\n",
    "    \"Couchbase Cloud\",\n",
    "    \"MongoDB Atlas\",\n",
    "    \"BaaS\",\n",
    "    \"Auth0\",\n",
    "    \"Cloudflare Workers\",\n",
    "    \"Kinsta Managed WordPress Hosting\",\n",
    "    \"Elastic Cloud\",\n",
    "    \"Integromat\",\n",
    "    \"Zapier\",\n",
    "    \"Cognito\",\n",
    "    \"S3\",\n",
    "    \"Oracle Cloud Infrastructure\",\n",
    "    \"IBM Watson\",\n",
    "    \"Azure DevOps\",\n",
    "    \"CloudStack\",\n",
    "    \"OpenStack\",\n",
    "    \"Scaleway\",\n",
    "    \"Jelastic\",\n",
    "    \"Backblaze B2\",\n",
    "    \"Linode Kubernetes Engine\",\n",
    "    \"Miro\",\n",
    "    \"Airtable\",\n",
    "    \"Quip\",\n",
    "    \"SurveyMonkey\",\n",
    "    \"Slack\",\n",
    "    \"Sentry\",\n",
    "    \"AppDynamics\",\n",
    "    \"New Relic\",\n",
    "    \"Grafana Cloud\",\n",
    "    \"Prometheus\",\n",
    "    \"CloudHealth\",\n",
    "    \"SaaSOptics\",\n",
    "    \"Zoho One\",\n",
    "    \"FreshBooks\",\n",
    "    \"Xero\",\n",
    "    \"QuickBooks Online\",\n",
    "    \"Wix\",\n",
    "    \"Webflow\",\n",
    "    \"Shopify Plus\",\n",
    "    \"BigCommerce\",\n",
    "    \"Adobe Experience Cloud\",\n",
    "    \"Veeva Vault\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "other_tools = [\n",
    "    \"Owasp\",\n",
    "    \"Zscaler\", \n",
    "    \"CrowdStrike\", \n",
    "    \"Rapid7\", \n",
    "    \"Defender VM\", \n",
    "    \"Qualys\", \n",
    "    \"Tenable\", \n",
    "    \"Nessus\", \n",
    "    \"Auth0\", \n",
    "    \"PingID\", \n",
    "    \"Azure AD\", \n",
    "    \"Okta\",\n",
    "    \"Cisco\",\"SSIS\",\n",
    "    \"Ansible\", \"Terraform\", \"Jenkins\", \"CircleCI\", \n",
    "    \"Travis CI\", \"GitLab CI\", \"AWS CodePipeline\", \"Chef\", \n",
    "    \"Puppet\", \"Octopus Deploy\", \"Helm\", \"Grafana\", \"Nagios\", \n",
    "    \"Datadog\", \"ELK Stack\", \"Kibana\", \n",
    "    \"GitHub Actions\", \"AppVeyor\", \"Codacy\", \"Tecton\", \"LaunchDarkly\", \n",
    "    \"Semaphore\", \"GitKraken\", \"Azure DevOps Server\", \"GitHub Pages\", \"Render\", \"Fly.io\", \n",
    "    \"Railway\", \"npm\", \"Yarn\", \"Composer\", \"Bower\", \"Gulp\", \n",
    "    \"Webpack\", \"Parcel\", \"Babel\", \"Grunt\", \"Vite\", \"Sentry\", \"SonarQube\",\"Kubernetes\",\"CVS\",\"Wordpress\",\"Apache Superset\",'SAS','Docker','QRadar', 'Securonix',\n",
    "       'Checkpoint', 'FireEye', 'ArcSight',\n",
    "       'NIST Cybersecurity Framework', 'Nessus', 'Wireshark',\n",
    "       'Palo Alto Networks', 'Burp Suite', 'Kali Linux', 'Trend Micro',\n",
    "       'Sophos', 'Responder', 'Metasploit', 'Nmap', 'Cisco ASA',\n",
    "    \"Qualys\",\n",
    "    \"Splunk\",\n",
    "    \"DataStage\",\n",
    "    \"spotfire\",\n",
    "    \"sap\",\n",
    "    \"Git\",\n",
    "    \"Office Suite\",\n",
    "    \"Trello\",\n",
    "    \"Asana\",\n",
    "    \"Jira\",\n",
    "    \"Confluence\",\n",
    "    \"Zoom\",\n",
    "    \"Adobe Creative Cloud\",\n",
    "    \"Figma\",\n",
    "    \"Canva\",\n",
    "    \"Miro\",\n",
    "    \"Power BI\",\n",
    "    \"Notion\",\n",
    "    \"Monday.com\",\n",
    "    \"Dropbox\",\n",
    "    \"Box\",\n",
    "    \"SharePoint\",\n",
    "    \"Evernote\",\n",
    "    \"GitHub\",\n",
    "    \"HubSpot\",\n",
    "    \"Mailchimp\",\n",
    "    \"Zapier\",\n",
    "    \"QuickBooks\",\n",
    "    \"Toggl\",\n",
    "    \"LastPass\",\n",
    "    \"Tableau\",\n",
    "    \"Google Analytics\",\n",
    "    \"SEMrush\",\n",
    "    \"JotForm\",\n",
    "    \"Basecamp\",\n",
    "    \"ClickUp\",\n",
    "    \"InVision\",\n",
    "    \"Lucidchart\",\n",
    "    \"Visio\",\n",
    "    \"PowerPoint\",\n",
    "    \"Google Slides\",\n",
    "    \"Balsamiq\",\n",
    "    \"MindMeister\",\n",
    "    \"XMind\",\n",
    "    \"Calendly\",\n",
    "    \"RescueTime\",\n",
    "    \"Adobe Acrobat\",\n",
    "    \"Mendeley\",\n",
    "    \"EndNote\",\n",
    "    \"Excel\",\n",
    "    \"Adobe Analytics\",\n",
    "    \"Looker\",\n",
    "    \"Adobe Creative Suite\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_df= pd.read_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\3day_indeed.csv\")\n",
    "glassdoor_df = pd.read_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\3day_glasdoor.csv\")\n",
    "jobstreet_df = pd.read_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\3day_jobstreet.csv\")\n",
    "indeed_df.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "glassdoor_df.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "jobstreet_df.drop(columns=[\"Unnamed: 0\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indeed_df.drop(columns=[\"Schedule\",\"Pages\",\"Condition\"],inplace=True)\n",
    "indeed_df.rename(columns={\"Year Of Experience\":\"Years Of Experience\"},inplace=True)\n",
    "indeed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glassdoor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glassdoor_df.rename(columns={\"Experience\":\"Years Of Experience\"},inplace=True)\n",
    "glassdoor_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobstreet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobstreet_df.drop(columns=[\"Description\",\"Page\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([indeed_df,glassdoor_df,jobstreet_df], ignore_index=True)\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_df = df.groupby('Short Job Title')['Short Job Title'].count()\n",
    "print(grouped_by_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prepping to start prediction\n",
    "df_izzhan_1 = pd.read_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\df_izzhan_1.csv\")\n",
    "df_izzhan_2 = pd.read_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\df_izzhan_2.csv\")\n",
    "df_izzhan=pd.concat([df_izzhan_1,df_izzhan_2])\n",
    "df_izzhan.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "df_izzhan=df_izzhan[df_izzhan[\"Short Job Title\"]!=\"Developer\"]\n",
    "\n",
    "\n",
    "#y_test_encoded = le.transform(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(df_izzhan[\"Short Job Title\"])\n",
    "label_dict={}\n",
    "unique_labels = df_izzhan[\"Short Job Title\"].unique()\n",
    "print(df_izzhan[\"Short Job Title\"].unique())\n",
    "for i in unique_labels :\n",
    "  label_dict[i]=le.transform([i])[0]\n",
    "\n",
    "print(\"Labels\")\n",
    "for i in label_dict:\n",
    "  print(i,label_dict[i])\n",
    "\n",
    "label_arr = [None] * len(label_dict)\n",
    "\n",
    "for i in label_dict:\n",
    "  label_arr[label_dict[i]] = i\n",
    "\n",
    "print(label_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load(\"C:\\\\Users\\\\ahmad\\\\Downloads\\\\job_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the developer short job title\n",
    "\n",
    "def predict_dev(Job_requirements,model_path):\n",
    "    Job_requirements_counts = vectorizer.transform([Job_requirements])\n",
    "    model = load(model_path)\n",
    "    prediction = model.predict(Job_requirements_counts)\n",
    "    #label = label_arr[prediction[0]]\n",
    "    label = le.inverse_transform(prediction)\n",
    "    return label[0]\n",
    "\n",
    "model_path = \"C:\\\\Users\\\\ahmad\\\\Downloads\\\\gbm_model.joblib\"\n",
    "#test=\" Opportunities for promotion Professional development Monday to Friday Software Engineer  Development  3 years  Preferred  Responsibilities   Executing full software development lifecycle  SDLC  including maintenance   Application systems integration with backend SCADA and or IT system  including usage of API  messaging and relevant technologies    Work in a scrum project with DevSecOps   Project documentation and communication such as requirements system analysis  design specifications and other relevant SDLC documentations  including presentation of works done  Requirement Skills   Experience with webapp DB development    Knowledge of algorithms and data structures    For the senior post  knowledge of microservices design  design patterns  and cloud based development   At least 3 to 10 yearsrecentexperience withReactJS and K8S    Experience with source control using Git    Experience with HTML5 CSS3 JQuery Bootstrap and NodeJS or Javascript  RESTful API   Experience with development using Flutter will be an added advantage   Able to communicate well in English and express design using UML notation and derive tasks backlog in SCRUM    For the senior post  Experience with team leading in agile scrum project with DevSecOps will be an added advantage    A fast learner  have a proactive attitude and an eye for quality works   A team player   inclusive mindset to collaborate effectively in a diverse team   Plus points will be additional advantage  o Experience with development on a public cloud  e.g  AWS  Microsoft Azure  GCP will be an advantage o Experience with microservices application development will be an advantage o Experience with microservices framework such as Molecular  and or Spring Boot o Experience in developing platform level features  such as configuration management  service discovery and routing  feature switch  A B testing  etc  o Experience with messaging  Kafka  AMQP  MQTT or other messaging technologies   Java and understand database schema design  o Experience with Maven  Nexus or Artifactory o Experience with CI CD systems  such as Bamboo  Jenkins o Scrum certified  and with experience in a Scrum project Job Type  ContractContract length  12 months Pay  RM6 500.00   RM7 500.00 per month Benefits  Schedule  Experience  \"\n",
    "#print(predict_dev(test,model_path))\n",
    "\n",
    "\n",
    "\n",
    "short_job_arr = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if(df.iloc[i][\"Short Job Title\"]==\"Developer\"):\n",
    "        short_job_arr.append(predict_dev(df.iloc[i][\"Job Requirements\"],model_path))\n",
    "    else:\n",
    "        short_job_arr.append(df.iloc[i][\"Short Job Title\"])\n",
    "df[\"Short Job Title\"]=short_job_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_df = df.groupby('Short Job Title')['Short Job Title'].count()\n",
    "print(grouped_by_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding Internship in the Job Type column\n",
    "\n",
    "job_type_arr = []\n",
    "\n",
    "for i in range (len(df)) :\n",
    "    if (\"intern\" in df.iloc[i][\"Job Title\"].lower() and \"international\" not in df.iloc[i][\"Job Title\"].lower()):\n",
    "        job_type_arr.append(\"Internship\")\n",
    "    else :\n",
    "        job_type_arr.append(df.iloc[i][\"Job Type\"])\n",
    "\n",
    "df[\"Job Type\"]= job_type_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a state column\n",
    "def get_state_and_city(place_name):\n",
    "    state_city_array=[]\n",
    "    # Set up the API URL with the input place name\n",
    "    url = f\"https://api.geoapify.com/v1/geocode/search?text={place_name},Malaysia&apiKey={geolocation_api_key}\"\n",
    "    \n",
    "    headers = CaseInsensitiveDict()\n",
    "    headers[\"Accept\"] = \"application/json\"\n",
    "    \n",
    "    # Send the GET request to the Geoapify API\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data[\"features\"]:\n",
    "            # Extract state and city information\n",
    "            state = data[\"features\"][0][\"properties\"].get(\"state\", \"State not found\")\n",
    "            city = data[\"features\"][0][\"properties\"].get(\"city\", \"City not found\")\n",
    "            state_city_array.append(state)\n",
    "            state_city_array.append(city)\n",
    "        else:\n",
    "            print(\"No results found for the specified location.\")\n",
    "            state_city_array.append(\"error\")\n",
    "            state_city_array.append(\"error\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        state_city_array.append(\"error\")\n",
    "        state_city_array.append(\"error\")\n",
    "\n",
    "    return state_city_array\n",
    "\n",
    "#location_cache = {}\n",
    "\n",
    "import json\n",
    "\n",
    "# Load the location_cache dictionary from a JSON file\n",
    "with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\location_cache.json\", \"r\") as f:\n",
    "    location_cache = json.load(f)\n",
    "\n",
    "def get_state_and_city_cached(place_name):\n",
    "    # Check if the location is already in the cache\n",
    "    if place_name in location_cache:\n",
    "        print(f\"Cache hit for {place_name}\")\n",
    "        return location_cache[place_name]\n",
    "    \n",
    "    # If not in the cache, make the API call\n",
    "    state_city_array = get_state_and_city(place_name)\n",
    "    \n",
    "    # Store the result in the cache for future use\n",
    "    location_cache[place_name] = state_city_array\n",
    "    \n",
    "    return state_city_array\n",
    "\n",
    "########################################################################\n",
    "\n",
    "state_arr=[]\n",
    "for i in range(len(df)):\n",
    "    temp_arr = []\n",
    "    if df.iloc[i][\"Location\"].lower() == \"remote\" or df.iloc[i][\"Location\"].lower() == \"malaysia\":\n",
    "        state_arr.append(\"Remote\")\n",
    "        continue\n",
    "    \n",
    "    # Use the cached version of the API call\n",
    "    temp_arr = get_state_and_city_cached(df.iloc[i][\"Location\"])\n",
    "    \n",
    "    if temp_arr[1] == \"Kuala Lumpur\":\n",
    "        state_arr.append(temp_arr[1])\n",
    "    else:\n",
    "        state_arr.append(temp_arr[0])\n",
    "\n",
    "    print(i + 1, \"row has been iterated\")\n",
    "\n",
    "\n",
    "df[\"State\"]=state_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "\n",
    "# Save the location_cache dictionary to a JSON file\n",
    "#with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\location_cache.json\", \"w\") as f:\n",
    "    #json.dump(location_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_df = df.groupby(['State'])['State'].count()\n",
    "# Set to None to display all rows\n",
    "print(grouped_by_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def convert_string_to_array(string):\n",
    "    try:\n",
    "        array = ast.literal_eval(string)\n",
    "        return array\n",
    "    except (ValueError, SyntaxError):\n",
    "        print(\"Invalid string format\")\n",
    "        return None\n",
    "\n",
    "skills_arr=[]\n",
    "for i in range (len(df)):\n",
    "    skills_arr.append (convert_string_to_array(df.iloc[i][\"Skills\"]))\n",
    "\n",
    "df[\"Skills\"]=skills_arr\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correcting tensorflow, PySpark, R and C in the table\n",
    "'''\n",
    "df_1 = df.copy()\n",
    "Skills_arr = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Make a copy of the original Skills array for each row\n",
    "    temp_arr = df.iloc[i][\"Skills\"][:]\n",
    "    \n",
    "    # Apply conditions to modify the copied list without affecting the original\n",
    "    if \"tensorflow\" in df.iloc[i][\"Job Requirements\"].lower():\n",
    "        temp_arr.append(\" Tensorflow \")\n",
    "        \n",
    "    elif \"pyspark\" in df.iloc[i][\"Job Requirements\"].lower():\n",
    "        temp_arr.append(\" PySpark \")\n",
    "        \n",
    "    elif (\"data\" not in df.iloc[i][\"Short Job Title\"].lower()) and \" R \" in temp_arr:\n",
    "        temp_arr.remove(\" R \")\n",
    "        \n",
    "    elif (\" C \" in temp_arr):\n",
    "        temp_arr.remove(\" C \")\n",
    "        \n",
    "    # Append the modified copy to Skills_arr\n",
    "    Skills_arr.append(temp_arr)\n",
    "\n",
    "# Update the Skills column with the new list of skills\n",
    "df_1[\"Skills\"] = Skills_arr\n",
    "df_1.head()\n",
    "'''\n",
    "\n",
    "df_1 = df.copy()\n",
    "Skills_arr=[]\n",
    "for i in range(len(df)):\n",
    "    temp_arr=[]\n",
    "    #if \"tensorflow\" in df.iloc[i][\"Job Requirements\"].lower():\n",
    "     #   temp_arr =df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\" Tensorflow \")\n",
    "    #if \"pyspark\" in df.iloc[i][\"Job Requirements\"].lower():\n",
    "     #   temp_arr =df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\" PySpark \")\n",
    "    if (\"data\" not in df.iloc[i][\"Short Job Title\"].lower()) and \" R \" in df.iloc[i][\"Skills\"]:\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.remove(\" R \")\n",
    "    #if (\" C \" in df.iloc[i][\"Skills\"]) :\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.remove(\" C \")\n",
    "    #if (\"scikit\" in df.iloc[i][\"Job Requirements\"].lower()) :\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\" Scikit \")\n",
    "    #if (\"wordpress\" in df.iloc[i][\"Job Requirements\"].lower()) :\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\" Wordpress \")\n",
    "    if(\"html5\" in df.iloc[i][\"Job Requirements\"].lower() and  \" HTML \"not in df.iloc[i][\"Skills\"] ):\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.append(\" HTML \")\n",
    "    if(\"css3\" in df.iloc[i][\"Job Requirements\"].lower() and  \" CSS \"not in df.iloc[i][\"Skills\"] ):\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.append(\" CSS \")\n",
    "    #if(\" D \") in df.iloc[i][\"Skills\"]:\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.remove(\" D \")\n",
    "    if(\"alibaba\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.append(\"Alibaba Cloud\")\n",
    "    #if(\"visual basic\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"Visual Basic\")\n",
    "    if(\"powerbi\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.append(\"Power BI\")\n",
    "    if (\" PowerBI \" in df.iloc[i][\"Skills\"]) :\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.remove(\" PowerBI \")\n",
    "    #if(\" mlflow \" in df.iloc[i][\"Skills\"]):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.remove(\" mlflow \")\n",
    "    #if(\"kafka\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"Kafka\")\n",
    "    #if(\"databricks\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"Databricks\")\n",
    "    #if(\"redshift\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"Redshift\")\n",
    "    #if(\"bigquery\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"Bigquery\")\n",
    "    #if(\"cisco\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"Cisco\")\n",
    "    #if(\" ssis \" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"SSIS\")\n",
    "    if(\" HIPAA \") in df.iloc[i][\"Skills\"]:\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.remove(\" HIPAA \")\n",
    "    if(\" Scrum \") in df.iloc[i][\"Skills\"]:\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.remove(\" Scrum \")\n",
    "    if (\"Cybersecurity\" == df.iloc[i][\"Short Job Title\"].lower()) and \" Swift \" in df.iloc[i][\"Skills\"]:\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.remove(\" Swift \")\n",
    "    if(\"nist csf\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "        temp_arr=df.iloc[i][\"Skills\"]\n",
    "        temp_arr.append(\"NIST Cybersecurity Framework\")\n",
    "    #if(\"iso27001\" in df.iloc[i][\"Job Requirements\"].lower()):\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.append(\"ISO 27001\")\n",
    "    #if(\"Django Rest Framework\") in df.iloc[i][\"Skills\"]:\n",
    "     #   temp_arr=df.iloc[i][\"Skills\"]\n",
    "      #  temp_arr.remove(\"Django Rest Framework\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    Skills_arr.append(df.iloc[i][\"Skills\"])\n",
    "\n",
    "df_1[\"Skills\"]=Skills_arr\n",
    "\n",
    "df_1.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\alljobs_df_2.csv\")\n",
    "df_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean salary column\n",
    "import re\n",
    "import numpy as np\n",
    "salary_col = []\n",
    "for i in range(len(df_1)):\n",
    "    if(df.iloc[i][\"Salary\"]==None):\n",
    "        salary_col.append(\"Not Specified\")\n",
    "    else:\n",
    "        salary_col.append(df.iloc[i][\"Salary\"])\n",
    "        \n",
    "salary_col = [\n",
    "    re.sub(r\"[^0-9\\- kK.]\", \"\", str(element))\n",
    "    for element in salary_col\n",
    "]\n",
    "salary_col = pd.Series(salary_col)\n",
    "#salary_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actual_salary(salary_str):\n",
    "    # Find all numbers, including those with decimals, with or without \"K\"\n",
    "    numbers = re.findall(r'\\d+\\.?\\d*K?', salary_str)\n",
    "    \n",
    "    # Convert each found number to a float, handling \"K\" as thousands\n",
    "    total = sum(float(num[:-1]) * 1000 if num.endswith('K') else float(num) for num in numbers)\n",
    "\n",
    "    # If there's only one number, return it as is\n",
    "    if len(numbers) == 1:\n",
    "        return total \n",
    "    \n",
    "    # Otherwise, divide the sum by 2\n",
    "    return total / 2\n",
    "\n",
    "\n",
    "for i in range(len(salary_col)):\n",
    "    if(salary_col[i]!=None):\n",
    "        salary_col[i]=actual_salary(salary_col[i])\n",
    "#salary_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(salary_col)) :\n",
    "    if (salary_col[i]<300 or \"hour\" in df.iloc[i][\"Salary\"]):\n",
    "        salary_col[i]=salary_col[i]*22*8\n",
    "    elif (salary_col[i]<1500 and  df.iloc[i][\"Job Type\"]!=\"Internship\") or \"day\" in df.iloc[i][\"Salary\"]:\n",
    "        salary_col[i]=salary_col[i]*22\n",
    "    elif salary_col[i]>45000 :\n",
    "        salary_col[i]=salary_col[i]/12\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define the data array\n",
    "data = salary_col\n",
    "\n",
    "# Define the start, end, and interval for bins\n",
    "start = 0\n",
    "end = 45000\n",
    "interval = 500\n",
    "\n",
    "# Generate bins from 0 to 45000 with intervals of 500\n",
    "bins = [(i, i + interval - 1) for i in range(start, end + 1, interval)]\n",
    "\n",
    "# Initialize a counter to store grouped data counts\n",
    "grouped_counts = Counter()\n",
    "\n",
    "# Classify each number in data by its bin range\n",
    "for number in data:\n",
    "    for bin_range in bins:\n",
    "        if bin_range[0] <= number <= bin_range[1]:\n",
    "            grouped_counts[f\"{bin_range[0]}-{bin_range[1]}\"] += 1\n",
    "            break\n",
    "\n",
    "# Print the grouped counts\n",
    "for range_label, count in grouped_counts.items():\n",
    "    print(f\"{range_label}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2=df_1.copy()\n",
    "df_2[\"Actual Salary\"]=salary_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_arrangement = ['Job ID', 'Job Title', 'Company Name', 'Location', 'Salary', 'Actual Salary', 'Job Type',\n",
    "       'Job Requirements', 'Skills', 'Years Of Experience', 'Short Job Title',\n",
    "       'State']\n",
    "\n",
    "df_2=df_2[new_column_arrangement]\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\alljobs_df_3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[\"Years Of Experience\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_df = df_2.groupby('Years Of Experience')['Years Of Experience'].count()\n",
    "print(grouped_by_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2[df_2[\"Years Of Experience\"]==\"+\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the experience column\n",
    "import math\n",
    "def word_to_number(word):\n",
    "    word_dict = {\n",
    "        'one': 1,\n",
    "        'two': 2,\n",
    "        'three': 3,\n",
    "        'four': 4,\n",
    "        'five': 5,\n",
    "        'six': 6,\n",
    "        'seven': 7,\n",
    "        'eight': 8,\n",
    "        'nine': 9,\n",
    "        'ten': 10,\n",
    "        'eleven': 11,\n",
    "        'twelve': 12,\n",
    "        'thirteen': 13,\n",
    "        'fourteen': 14,\n",
    "        'fifteen': 15,\n",
    "        'sixteen': 16,\n",
    "        'seventeen': 17,\n",
    "        'eighteen': 18,\n",
    "        'nineteen': 19,\n",
    "        'twenty': 20\n",
    "    }\n",
    "    \n",
    "    word = word.lower()  # Convert to lowercase to make the function case-insensitive\n",
    "    return word_dict.get(word, \"Invalid input\")\n",
    "\n",
    "def has_plus(something):\n",
    "    if \"+\" in something:\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "def has_numbers(something):\n",
    "    return any(char.isdigit() for char in something)\n",
    "\n",
    "def only_numbers(something):\n",
    "    return ''.join(char for char in something if char.isdigit())\n",
    "\n",
    "def is_float(something):\n",
    "    try:\n",
    "        something=float(something)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "experience_arr = []\n",
    "\n",
    "for i in range (len(df_2)):\n",
    "    temp_string = df_2.iloc[i][\"Years Of Experience\"]\n",
    "    if(has_plus(temp_string)):\n",
    "        temp_string = temp_string.replace(\"+\",\"\")\n",
    "    if(is_float(temp_string)):\n",
    "        temp_float=float(temp_string)\n",
    "        temp_float=math.floor(temp_float)\n",
    "        temp_int=round(temp_float)\n",
    "        temp_string = str(temp_float)\n",
    "    if(word_to_number(temp_string) != \"Invalid input\"):\n",
    "        temp_int = word_to_number(temp_string)\n",
    "        temp_string=str(temp_int)\n",
    "    if(has_numbers(temp_string)):\n",
    "        temp_string = only_numbers(temp_string)\n",
    "        if(int(temp_string)>20):\n",
    "            temp_string=\"0\"\n",
    "    else:\n",
    "        temp_string=\"0\"\n",
    "\n",
    "    experience_arr.append(temp_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=df_2.copy()\n",
    "df_3[\"Years Of Experience\"]=experience_arr\n",
    "df_3.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3[\"Years Of Experience\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({\"before_cleaned\":df_2[\"Years Of Experience\"],\"After_cleaned\":df_3[\"Years Of Experience\"]}).to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\experience_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dots in skills column\n",
    "def remove_dots(skills):\n",
    "    if(len(skills)==0):\n",
    "          return []\n",
    "    return [skill.replace('.', '') for skill in skills]\n",
    "\n",
    "Skills_arr=[]\n",
    "\n",
    "for i in range(len(df_3)):\n",
    "        Skills_arr.append(remove_dots(df_3.iloc[i][\"Skills\"]))\n",
    "\n",
    "df_3[\"Skills\"]=Skills_arr\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing 0's to Nan values\n",
    "actual_salary=[]\n",
    "Years_Of_Experience=[]\n",
    "Skills_array=[]\n",
    "for i in range(len(df_3)):\n",
    "    if(df_3.iloc[i][\"Actual Salary\"]==0.0):\n",
    "        actual_salary.append(np.nan)\n",
    "    else :\n",
    "        actual_salary.append(df_3.iloc[i][\"Actual Salary\"])\n",
    "\n",
    "for i in range(len(df_3)):\n",
    "    if(df_3.iloc[i][\"Years Of Experience\"]==\"0\"):\n",
    "        Years_Of_Experience.append(np.nan)\n",
    "    else:\n",
    "        Years_Of_Experience.append(df_3.iloc[i][\"Years Of Experience\"])\n",
    "\n",
    "for i in range(len(df_3)):\n",
    "    if(len(df_3.iloc[i][\"Skills\"])==0):\n",
    "        Skills_array.append(np.nan)\n",
    "    else:\n",
    "        Skills_array.append(df_3.iloc[i][\"Skills\"])\n",
    "\n",
    "df_3[\"Actual Salary\"]=actual_salary\n",
    "df_3[\"Years Of Experience\"]=Years_Of_Experience\n",
    "df_3[\"Skills\"]=Skills_array\n",
    "df_3.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_3.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\alljobs_df_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=df_3[df_3[\"Short Job Title\"]!= \"Cybersecurity\"]\n",
    "df_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_3.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\alljobs_df_8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a new table to handle multivalued columns (skills column)\n",
    "df_4=df_3.copy()\n",
    "job_id_arr = df_4[\"Job ID\"]\n",
    "Skills_arr = df_4[\"Skills\"]\n",
    "\n",
    "df_skills = pd.DataFrame({\n",
    "    \"Job ID\":job_id_arr,\n",
    "    \"Skills\":Skills_arr\n",
    "})\n",
    "\n",
    "df_skills=df_skills.explode(\"Skills\")\n",
    "df_skills.dropna(inplace=True)\n",
    "df_skills.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.drop(columns=[\"Skills\"],inplace=True)\n",
    "#df_3.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\alljobs_df_9.csv\")\n",
    "df_3.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\3day_alljobs_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3[(df_3[\"Short Job Title\"]==\"Data Engineer\") & (df_3[\"Job Type\"]==\"Internship\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "skills_arr=[]\n",
    "for i in range(len(df_skills)):\n",
    "    if(pd.notna(df_skills.iloc[i][\"Skills\"])):\n",
    "        skills_arr.append(df_skills.iloc[i][\"Skills\"].strip())  # Strip only leading and trailing spaces\n",
    "    else:\n",
    "        skills_arr.append(np.nan)\n",
    "\n",
    "df_skills[\"Skills\"] = skills_arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_df = df_3.groupby('State')['State'].count()\n",
    "print(grouped_by_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#df_skills.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\skills_table3.csv\")\n",
    "df_skills.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\3day_skills_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_skills=df_skills[\"Skills\"].unique()\n",
    "unique_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_skill_type(skill):\n",
    "    if skill.lower() in programming_languages.str.lower().values:\n",
    "        return \"Programming Language\"\n",
    "    elif skill.lower() in databases.str.lower().values:\n",
    "        return \"Database\"\n",
    "    elif skill.lower() in frameworks.str.lower().values:\n",
    "        return \"Frameworks and Libraries\"\n",
    "    elif skill.lower() in cloud_service_providers.str.lower().values:\n",
    "        return \"Cloud Service Providers\"\n",
    "    elif skill.lower() in other_tools.str.lower().values:\n",
    "        return \"Other tools\"\n",
    "    else :\n",
    "        return \"idk bro\"\n",
    "\n",
    "\n",
    "programming_languages=pd.Series(programming_languages)\n",
    "databases = pd.Series(databases)\n",
    "frameworks = pd.Series(frameworks)\n",
    "cloud_service_providers = pd.Series(cloud_service_providers)\n",
    "other_tools = pd.Series(other_tools)\n",
    "skill_type=[]\n",
    "\n",
    "unique_skills=pd.Series(unique_skills)\n",
    "unique_skills.dropna()\n",
    "\n",
    "for i in range (len(unique_skills)) :\n",
    "    if(pd.notna(unique_skills[i])):\n",
    "        skill_type.append(def_skill_type(unique_skills[i]))\n",
    "    else:\n",
    "        skill_type.append(\"idk bro\")\n",
    "\n",
    "df_skill_dim = pd.DataFrame({\"Skill\":unique_skills,\"Skill Type\":skill_type})\n",
    "df_skill_dim = df_skill_dim[df_skill_dim[\"Skill Type\"] != \"idk bro\"]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skill_dim.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skill_dim.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\3day_skill_dim.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by \"Skill Type\" to review the skills under each category for potential misclassifications\n",
    "pd.set_option('display.max_rows', None)\n",
    "grouped_skills = df_skill_dim.groupby(\"Skill Type\")[\"Skill\"].apply(list)\n",
    "# Display grouped skills to assess potential misclassifications\n",
    "grouped_skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_skill_dim[\"Skill\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_skill_dim[\"Skill\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for skill_type, skills_list in grouped_skills.items():\n",
    "    print(f\"Skill Type: {skill_type}\")\n",
    "    print(f\"Skills: {skills_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_4.drop(columns=[\"Skills\"],inplace=True)\n",
    "df_merged = pd.merge(df_skills,df_4,how=\"left\",on=\"Job ID\")\n",
    "df_merged = df_merged[['Job ID', 'Skills', 'Short Job Title']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "grouped_skills = df_merged.groupby([\"Short Job Title\",\"Skills\"])[\"Skills\"].count().sort_values(ascending=False)\n",
    "# Display grouped skills to assess potential misclassifications\n",
    "grouped_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the output\n",
    "formatted_date = now.strftime(\"%Y-%m-%d\")\n",
    "formatted_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(f\"This code is executed on: {formatted_date}\")\n",
    "print(f\"Current time: {formatted_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
