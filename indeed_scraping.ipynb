{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "programming_languages = [\n",
    "    \"Python\", \"JavaScript\", \"Java\", \"C#\", \"C++\", \"PHP\", \"Ruby\", \"Swift\",\n",
    "    \"Golang\", \"TypeScript\", \"R\", \"Kotlin\", \"HTML\", \"CSS\", \"SQL\", \"Rust\", \"Dart\",\n",
    "    \"Scala\", \"Perl\", \"Haskell\", \"Elixir\", \"Lua\", \"Bash\",\n",
    "    \"Objective-C\", \"Groovy\", \"F#\", \"Visual Basic\", \"MATLAB\", \"Assembly Language\",\n",
    "    \"Julia\", \"Cobol\", \"Fortran\", \"Erlang\", \"Prolog\", \"Crystal\", \"Tcl\", \"Lisp\",\n",
    "    \"ActionScript\", \"Apex\", \"Clojure\", \"Smalltalk\", \"Solidity\", \"NIM\", \n",
    "    \"OCaml\", \"Q#\", \"Ceylon\", \"VBScript\", \"Awk\", \"Racket\",\n",
    "    \"ABAP\", \"XSLT\", \"Lasso\", \"Max\", \"PostScript\", \n",
    "    \"Simulink\", \"Io\", \"Datalog\", \"Pliant\", \"J\", \"GAMS\", \"Gherkin\", \"SuperCollider\",\n",
    "    \"Sed\", \"PASCAL\"\n",
    "]\n",
    "\n",
    "for i in range(len(programming_languages)) :\n",
    "    if \".\" in programming_languages[i] :\n",
    "        programming_languages.append(programming_languages[i].replace(\".\",\"\"))\n",
    "\n",
    "databases = [\n",
    "    \"MySQL\",\n",
    "    \"PostgreSQL\",\n",
    "    \"SQLite\",\n",
    "    \"MongoDB\",\n",
    "    \"SQL Server\",\n",
    "    \"Oracle\",\n",
    "    \"Redis\",\n",
    "    \"Cassandra\",\n",
    "    \"MariaDB\",\n",
    "    \"DB2\",\n",
    "    \"Amazon DynamoDB\",\n",
    "    \"Firebase Realtime\",\n",
    "    \"Elasticsearch\",\n",
    "    \"Azure SQL\",\n",
    "    \"SAP HANA\",\n",
    "    \"CockroachDB\",\n",
    "    \"Google Cloud Firestore\",\n",
    "    \"Couchbase\",\n",
    "    \"Neo4j\",\n",
    "    \"HBase\",\n",
    "    \"Apache Hive\",\n",
    "    \"Teradata\",\n",
    "    \"Apache Cassandra\",\n",
    "    \"ArangoDB\",\n",
    "    \"RavenDB\",\n",
    "    \"Citus\",\n",
    "    \"TimescaleDB\",\n",
    "    \"OrientDB\",\n",
    "    \"Titan\",\n",
    "    \"TokuMX\",\n",
    "    \"VoltDB\",\n",
    "    \"Memcached\",\n",
    "    \"OpenTSDB\",\n",
    "    \"Dgraph\",\n",
    "    \"MarkLogic\",\n",
    "    \"Apache Drill\",\n",
    "    \"CouchDB\",\n",
    "    \"SAP IQ\",\n",
    "    \"NuoDB\",\n",
    "    \"Amazon Aurora\",\n",
    "    \"Azure Cosmos DB\",\n",
    "    \"CrateDB\",\n",
    "    \"Greenplum\",\n",
    "    \"Pivotal GemFire\",\n",
    "    \"EventStore\",\n",
    "    \"SQL Anywhere\",\n",
    "    \"DataStax Enterprise\",\n",
    "    \"AllegroGraph\",\n",
    "    \"Presto\",\n",
    "    \"Amazon Redshift\",\n",
    "    \"Informix\",\n",
    "    \"Apache Kudu\",\n",
    "    \"Sybase\",\n",
    "    \"Firebird\",\n",
    "    \"Apache Derby\",\n",
    "    \"SQLite\",\n",
    "    \"MaxDB\",\n",
    "    \"Teradata Aster\",\n",
    "    \"Vertica\",\n",
    "    \"Linterra\",\n",
    "    \"RocksDB\",\n",
    "    \"ClickHouse\",\n",
    "    \"Hadoop HDFS\",\n",
    "    \"Joomla\",\n",
    "    \"MSSQL\",\n",
    "    \"Xbase\",\n",
    "    \"Zebra\",\n",
    "    \"Wikidata\",\n",
    "    \"QlikView\",\n",
    "    \"Druid\",\n",
    "    \"Apache Phoenix\",\n",
    "    \"HSQLDB\",\n",
    "    \"Realm\",\n",
    "    \"CockroachDB\",\n",
    "    \"QLDB\",\n",
    "    \"Tarantool\",\n",
    "    \"Couchbase\",\n",
    "    \"Sphinx\",\n",
    "    \"InterBase\",\n",
    "    \"PouchDB\",\n",
    "    \"RavenDB\",\n",
    "    \"Pivotal Greenplum\",\n",
    "    \"TQL\",\n",
    "    \"Blazegraph\",\n",
    "    \"Netezza\",\n",
    "    \"Exasol\",\n",
    "    \"Coda\",\n",
    "    \"Qlik Sense\",\n",
    "    \"Linterra\",\n",
    "    \"Glean\",\n",
    "    \"OpenCensus\",\n",
    "    \"Snowflake\",\n",
    "    \"Metabase\"\n",
    "]\n",
    "for i in range(len(databases)) :\n",
    "    if \".\" in databases[i] :\n",
    "        print(databases[i])\n",
    "        databases.append(databases[i].replace(\".\",\"\"))\n",
    "\n",
    "frameworks = [\n",
    "    # Web Development\n",
    "    \"Kafka\",\n",
    "    \"SwiftUI\",\"Node.js\",\"React\", \"Angular\", \"Vue.js\", \"Django\", \"Flask\", \"Ruby on Rails\", \"Express.js\", \n",
    "    \"ASP.NET\", \"Spring\", \"Laravel\", \"Symfony\", \"FastAPI\", \"Svelte\", \"Backbone.js\", \n",
    "    \"CodeIgniter\", \"NestJS\", \"Meteor\", \"Pyramid\", \"Phoenix\", \n",
    "    \"Ionic\", \"Bootstrap\", \"Bulma\", \"Materialize\", \"Tailwind CSS\", \n",
    "    \"Ember.js\", \"Next.js\", \"Nuxt.js\", \"Gatsby\", \"Zope\", \"Sinatra\", \"JSP\", \n",
    "    \"Play Framework\", \"Tornado\", \"Web2py\", \"Sequelize\", \"Knex.js\", \"Deno\", \n",
    "    \"Sanity\", \"Strapi\", \"GraphQL\", \"RESTful API\", \"Jekyll\", \"Hugo\", \"Docusaurus\", \n",
    "    \"Aurelia\", \"Mithril\", \"Quasar\", \"Alpine.js\", \"Elm\", \"ClojureScript\", \n",
    "    \"PicoCMS\", \"Ant Design\", \"PrimeNG\", \"Semantic UI\", \"jQuery\", \n",
    "    \"Preact\", \"Turbo\", \"AppSync\", \"Flask-SocketIO\", \"Vapor\", \n",
    "    \"Jersey\", \"Vaadin\", \"Pico\", \"YII\", \"Tiki Wiki\", \"Phalcon\", \"Hapi.js\", \n",
    "    \"Koa.js\", \"Restify\", \"Slim\", \"Silex\", \"Liquid\", \"BootstrapVue\", \"Fomantic UI\", \n",
    "    \"Aurelia\", \"Chai\", \"Sass\", \"PostCSS\", \"jQuery UI\", \"Kendo UI\", \n",
    "    \"Dojo\", \"Webix\", \"Gijgo\", \"Pikaday\", \"FullCalendar\", \"Handsontable\", \n",
    "    \"GrapeJS\", \"Frappe\", \"Phabricator\", \"CakePHP\", \"Nette\", \"Yii2\", \"SilverStripe\", \n",
    "    \"OroCRM\", \"TYPO3\", \"CouchCMS\", \"Concrete5\", \"Grav\", \"Kirby\", \"OctoberCMS\", \n",
    "    \"ProcessWire\", \"MODX\", \"Craft CMS\", \"Pimcore\", \"Bolt\", \"Umbraco\", \"DotNetNuke\",\n",
    "\n",
    "    # Mobile Development\n",
    "    \"React Native\", \"Flutter\", \"Xamarin\", \"Ionic\", \"Apache Cordova\", \"USwiftI\", \n",
    "    \"Kotlin Multiplatform Mobile\", \"PhoneGap\", \"NativeScript\", \"Sencha Touch\", \n",
    "    \"Appcelerator Titanium\", \"Framework7\", \"Unity\", \"Cocos2d-x\", \"Fusetools\", \n",
    "    \"NativeBase\", \"Quasar Framework\", \"Onsen UI\", \"Cordova\", \"ReactXP\", \n",
    "    \"Tauri\", \"Kivy\", \"PyQt5\", \"wxPython\", \"BeeWare\", \"Gluon\", \"Crosswalk\", \n",
    "    \"Fyn\", \"Material Components for Android\", \"Robolectric\", \"Apache Felix\", \n",
    "    \"Zygote\", \"UI Automator\", \"MonkeyRunner\", \"Firebase UI\", \"Codename One\", \n",
    "    \"Nativescript-Vue\", \"RxJava\", \"Dagger\", \"ButterKnife\", \"Retrofit\", \"Volley\", \n",
    "    \"OkHttp\", \"RxAndroid\", \"Firebase Cloud Messaging\", \"Fastlane\", \"Swift Package Manager\",\n",
    "\n",
    "    # Game Development\n",
    "    \"Unity\", \"Unreal Engine\", \"Godot\", \"Cocos2d\", \"CryEngine\", \"GameMaker Studio\", \n",
    "    \"Phaser\", \"LibGDX\", \"Defold\", \"Ren'Py\", \"SpriteKit\", \"Cocos2d-x\", \n",
    "    \"Pygame\", \"MonoGame\", \"JMonkeyEngine\", \"GameSalad\", \"Havok\", \"Fmod\", \"Unity3D\", \n",
    "    \"CryEngine\", \"Torque3D\", \"PlayCanvas\", \"Blend4Web\", \"Panda3D\", \"CopperLicht\", \n",
    "    \"Ogre3D\", \"Three.js\", \"Babylon.js\", \"Fusio\", \"LÖVE\", \"BGE\", \"Cinder\", \n",
    "    \"Game Framework\", \"Wave Engine\", \"PICO-8\", \"Phaser Editor\", \"GDevelop\", \n",
    "    \"Cocos Creator\", \"Chocolat\", \"Bevy\", \"Flixel\", \"Luxe\", \"Leadwerks\", \n",
    "    \"Allegro\", \"GML\", \"Squirrel\", \"Visual3D\", \"Sunburn\", \"C4\", \"Zenject\",\n",
    "\n",
    "    # Data Science and Machine Learning\n",
    "    \"PySpark\",\"Tensorflow\", \"Keras\", \"PyTorch\", \"Scikit\", \"Pandas\", \"NumPy\", \n",
    "    \"Matplotlib\", \"Seaborn\", \"Spark\", \"Dask\", \"H2O.ai\", \"Apache Flink\", \n",
    "    \"Hadoop\", \"OpenCV\", \"NLTK\", \"spaCy\", \"FastAI\", \"XGBoost\", \"LightGBM\", \n",
    "    \"CatBoost\", \"MLflow\", \"Airflow\", \"PyCaret\", \"Dask-ML\", \"Haystack\", \n",
    "    \"Shiny\", \"Dash\", \"Plotly\", \"Streamlit\", \"Bokeh\", \"TensorBoard\", \n",
    "    \"Chainer\", \"Pytorch Lightning\", \"TPOT\", \"DataRobot\", \"AutoML\", \"TPOT\", \n",
    "    \"Optuna\", \"Ray\", \"FastText\", \"gensim\", \"Featuretools\", \"Weka\", \n",
    "    \"KNIME\", \"Alteryx\", \"Datarobot\", \"Metaflow\", \"Jupyter Notebook\", \n",
    "     \"Dash\", \"Orange3\", \"DeepSpeed\", \"Fairlearn\", \"FiftyOne\",\n",
    "\n",
    "    # DevOps and CI/CD\n",
    "     \"Babel\",\"Vite\",\"Parcel\",\"Webpack\",\"Grunt\",\n",
    "\n",
    "    # Testing Frameworks\n",
    "    \"JUnit\", \"pytest\", \"Selenium\", \"Mocha\", \"Chai\", \"Jasmine\", \"Cypress\", \n",
    "    \"TestNG\", \"RSpec\", \"Jest\", \"Karma\", \"Puppeteer\", \"Robot Framework\", \n",
    "    \"Protractor\", \"Postman\", \"Cucumber\", \"JUnit\", \"NUnit\", \"Vitest\", \n",
    "    \"Enzyme\", \"Cypress\", \"Supertest\", \"Mocha\", \"Mochawesome\", \"Pytest-bdd\", \n",
    "    \"Gatling\", \"Locust\", \"Trestle\", \"Avro\", \"WireMock\", \"Karate\", \n",
    "    \"Selenium Grid\", \"Applitools\", \"Gauge\", \"Cypress\", \"RestAssured\", \n",
    "    \"Flask-Testing\", \"Flask-RESTPlus\", \"Playwright\", \"SpecFlow\", \"Codacy\", \n",
    "    \"SonarQube\", \"Selenium IDE\",\n",
    "\n",
    "    # Desktop Applications\n",
    "    \"Electron\", \"Qt\", \"GTK\", \"JavaFX\", \"wxWidgets\", \"Avalonia\", \"Flutter Desktop\", \n",
    "    \"Nw.js\", \"Xamarin.Forms\", \"React Native Windows\", \"Tauri\", \"Electron Forge\", \n",
    "    \"PyQt\", \"Tkinter\", \"Kivy\", \"PySide\", \"WinForms\", \"MFC\", \"GTK\", \"UWP\", \n",
    "    \"Avalonia UI\", \"JavaFX\", \"Flutter\", \"Blazor\", \"Uno Platform\", \"WPF\", \n",
    "    \"Chocolat\", \"AppKit\", \"Mac Catalyst\", \"PyQt5\", \"Gnome\", \"Electron React\", \n",
    "    \"Cocoa\", \"Xamarin\", \"Delphi\", \"Qt Creator\", \"QT Quick\", \"Swing\", \n",
    "    \"Java Swing\", \"Pygame\", \"Avalonia\", \"VCL\",\n",
    "\n",
    "    # Other Frameworks and Libraries\n",
    "\n",
    "    \"FAIR\",\"KOBIT\", \"RabbitMQ\", \"Celery\", \"OpenShift\", \"Nginx\", \"Apache HTTP Server\" , \"SendGrid\", \"Twilio\", \"Stripe\", \"Socket.io\", \n",
    "    \"GraphQL\", \"gRPC\", \"Pusher\", \"Flask-RESTful\", \n",
    "    \"Jupyter Notebook\", \"Apache Thrift\", \"Hapi.js\", \"React Query\", \"Apollo Client\", \n",
    "    \"WebAssembly\", \"RxJS\", \"Flux\", \"MobX\", \"Gulp\", \"Webpack\", \"Parcel\", \n",
    "    \"Babel\", \"Grunt\", \"Vite\", \"Puppeteer\", \"Sass\", \"PostCSS\", \"Frappe\", \"Django REST Framework\", \"Netty\", \n",
    "    \"Spring Boot\", \"Play Framework\", \"Dropwizard\", \"Java EE\", \"Vaadin\", \n",
    "    \"Grails\", \"Avert\", \"Flyway\", \"JOOQ\", \"Spring Data\", \"MicroProfile\", \n",
    "    \"Ktor\", \"Vert.x\", \"Gson\", \"Jackson\", \"jOOQ\", \"Apache Ant\", \"Jenkins\", \n",
    "    \"Concourse\", \"Mercurial\", \"Bazaar\", \n",
    "    \"TFS\", \"Subversion\", \"Plastic SCM\", \"FogBugz\", \"Phabricator\", \"Helix Core\", \n",
    "    \"Gitea\", \"Gogs\", \"SourceGear Vault\", \"SmartSVN\", \"TortoiseSVN\", \n",
    "    \"Fossil\", \"SVN\", \"Aegis\",\n",
    "\n",
    "    # Additional Frameworks\n",
    "    \"ISO 27001\",\n",
    "\"MITRE ATT\",\n",
    "\"COBIT\",\n",
    "\"PCI DSS\",\n",
    "\"SOC 2\",\n",
    "\"GDPR\",\n",
    "\"ITIL\",\n",
    "\"CMMI\",\n",
    "\"Cis Controls\",\n",
    "    \"Spring Cloud\", \"Spring Security\", \"Spring MVC\", \"Spring Integration\", \n",
    "    \"Spring Batch\", \"Apache Camel\", \"Apache Shiro\", \"Canoe\", \"Libuv\", \n",
    "    \"ASP.NET MVC\", \"ASP.NET Core\", \"Apache Cordova\", \"Xamarin\", \"OSGi\", \n",
    "    \"React Query\", \"SWR\", \"Apollo Client\", \"Puppeteer\", \"Cheerio\", \n",
    "    \"Bootstrap\", \"Fomantic UI\", \"Materialize\", \"PrimeReact\", \n",
    "    \"Ant Design\", \"Semantic UI\", \"Spectre.css\", \"UIKit\", \"Material Design Lite\", \n",
    "    \"Milligram\", \"Skeleton\", \"Blaze UI\", \"Fomantic UI\", \"Semantic UI React\", \n",
    "    \"Stylus\", \"Foundation Sites\", \"HTML5 Boilerplate\", \"Normalize.css\", \n",
    "    \"PostCSS\", \"CSS Modules\", \"CSS-in-JS\", \"Styled Components\", \n",
    "    \"Emotion\", \"JSS\", \"Radium\", \"Shadow DOM\", \"LitElement\", \"HyperHTML\", \n",
    "    \"html.js\", \"Gatsby\", \"Next.js\", \"Nuxt.js\", \"Hugo\", \"Jekyll\", \"Docusaurus\", \n",
    "    \"Scully\", \"Sapper\", \"Gatsby\", \"Middleman\", \"Grunt\", \"Gulp\", \"npm\", \n",
    "    \"Yarn\", \"Webpack\", \"Parcel\", \"Vite\", \"Fly\", \"Zola\", \"Ziggy\", \"TiddlyWiki\", \n",
    "    \"Sphinx\", \"MkDocs\", \"Doxygen\", \"HDoc\", \"Doxygen\", \n",
    "    \"Javadoc\", \"Sphinx\", \"Hugo\", \"Jekyll\", \"MkDocs\"\n",
    "]\n",
    "\n",
    "\n",
    "for i in range(len(frameworks)) :\n",
    "    if \".\" in frameworks[i] :\n",
    "        frameworks.append(frameworks[i].replace(\".\",\"\"))\n",
    "\n",
    "\n",
    "cloud_service_providers = [\n",
    "    \"Bigquery\",\n",
    "    \"Redshift\",\n",
    "    \"Databricks\",\n",
    "    \"AWS\",\n",
    "    \"Azure\",\n",
    "    \"GCP\",\n",
    "    \"IBM Cloud\",\n",
    "    \"Oracle Cloud\",\n",
    "    \"Salesforce\",\n",
    "    \"DigitalOcean\",\n",
    "    \"Linode\",\n",
    "    \"Vultr\",\n",
    "    \"Heroku\",\n",
    "    \"Rackspace\",\n",
    "    \"Cloudflare\",\n",
    "    \"Red Hat OpenShift\",\n",
    "    \"SAP Cloud Platform\",\n",
    "    \"Alibaba Cloud\",\n",
    "    \"Mendix\",\n",
    "    \"Cisco Cloud\",\n",
    "    \"Cloudways\",\n",
    "    \"Trello\",\n",
    "    \"Zoho\",\n",
    "    \"Atlassian Cloud\",\n",
    "    \"Smartsheet\",\n",
    "    \"Firebase\",\n",
    "    \"Contentful\",\n",
    "    \"Shopify\",\n",
    "    \"Zendesk\",\n",
    "    \"Fastly\",\n",
    "    \"Akamai\",\n",
    "    \"SiteGround\",\n",
    "    \"InMotion Hosting\",\n",
    "    \"WP Engine\",\n",
    "    \"GreenGeeks\",\n",
    "    \"A2 Hosting\",\n",
    "    \"HostGator\",\n",
    "    \"iPage\",\n",
    "    \"Bluehost\",\n",
    "    \"Liquid Web\",\n",
    "    \"DreamHost\",\n",
    "    \"Kinsta\",\n",
    "    \"CloudSigma\",\n",
    "    \"OVHcloud\",\n",
    "    \"CenturyLink Cloud\",\n",
    "    \"Gandi\",\n",
    "    \"Google Workspace\",\n",
    "    \"Microsoft 365\",\n",
    "    \"MaxCompute\",\n",
    "    \"Clever Cloud\",\n",
    "    \"Render\",\n",
    "    \"Platform.sh\",\n",
    "    \"Back4App\",\n",
    "    \"Kinvey\",\n",
    "    \"GCP Firebase\",\n",
    "    \"Bitbucket\",\n",
    "    \"Vercel\",\n",
    "    \"Netlify\",\n",
    "    \"Glitch\",\n",
    "    \"Heroku Postgres\",\n",
    "    \"Linode Block Storage\",\n",
    "    \"Cloudian\",\n",
    "    \"Pivotal Cloud Foundry\",\n",
    "    \"Couchbase Cloud\",\n",
    "    \"MongoDB Atlas\",\n",
    "    \"BaaS\",\n",
    "    \"Auth0\",\n",
    "    \"Cloudflare Workers\",\n",
    "    \"Kinsta Managed WordPress Hosting\",\n",
    "    \"Elastic Cloud\",\n",
    "    \"Integromat\",\n",
    "    \"Zapier\",\n",
    "    \"Cognito\",\n",
    "    \"S3\",\n",
    "    \"Oracle Cloud Infrastructure\",\n",
    "    \"IBM Watson\",\n",
    "    \"Azure DevOps\",\n",
    "    \"CloudStack\",\n",
    "    \"OpenStack\",\n",
    "    \"Scaleway\",\n",
    "    \"Jelastic\",\n",
    "    \"Backblaze B2\",\n",
    "    \"Linode Kubernetes Engine\",\n",
    "    \"Miro\",\n",
    "    \"Airtable\",\n",
    "    \"Quip\",\n",
    "    \"SurveyMonkey\",\n",
    "    \"Slack\",\n",
    "    \"Sentry\",\n",
    "    \"AppDynamics\",\n",
    "    \"New Relic\",\n",
    "    \"Grafana Cloud\",\n",
    "    \"Prometheus\",\n",
    "    \"CloudHealth\",\n",
    "    \"SaaSOptics\",\n",
    "    \"Zoho One\",\n",
    "    \"FreshBooks\",\n",
    "    \"Xero\",\n",
    "    \"QuickBooks Online\",\n",
    "    \"Wix\",\n",
    "    \"Webflow\",\n",
    "    \"Shopify Plus\",\n",
    "    \"BigCommerce\",\n",
    "    \"Adobe Experience Cloud\",\n",
    "    \"Veeva Vault\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "other_tools = [\n",
    "    \"Owasp\",\n",
    "    \"Zscaler\", \n",
    "    \"CrowdStrike\", \n",
    "    \"Rapid7\", \n",
    "    \"Defender VM\", \n",
    "    \"Qualys\", \n",
    "    \"Tenable\", \n",
    "    \"Nessus\", \n",
    "    \"Auth0\", \n",
    "    \"PingID\", \n",
    "    \"Azure AD\", \n",
    "    \"Okta\",\n",
    "    \"Cisco\",\"SSIS\",\n",
    "    \"Ansible\", \"Terraform\", \"Jenkins\", \"CircleCI\", \n",
    "    \"Travis CI\", \"GitLab CI\", \"AWS CodePipeline\", \"Chef\", \n",
    "    \"Puppet\", \"Octopus Deploy\", \"Helm\", \"Grafana\", \"Nagios\", \n",
    "    \"Datadog\", \"ELK Stack\", \"Kibana\", \n",
    "    \"GitHub Actions\", \"AppVeyor\", \"Codacy\", \"Tecton\", \"LaunchDarkly\", \n",
    "    \"Semaphore\", \"GitKraken\", \"Azure DevOps Server\", \"GitHub Pages\", \"Render\", \"Fly.io\", \n",
    "    \"Railway\", \"npm\", \"Yarn\", \"Composer\", \"Bower\", \"Gulp\", \n",
    "    \"Webpack\", \"Parcel\", \"Babel\", \"Grunt\", \"Vite\", \"Sentry\", \"SonarQube\",\"Kubernetes\",\"CVS\",\"Wordpress\",\"Apache Superset\",'SAS','Docker','QRadar', 'Securonix',\n",
    "       'Checkpoint', 'FireEye', 'ArcSight',\n",
    "       'NIST Cybersecurity Framework', 'Nessus', 'Wireshark',\n",
    "       'Palo Alto Networks', 'Burp Suite', 'Kali Linux', 'Trend Micro',\n",
    "       'Sophos', 'Responder', 'Metasploit', 'Nmap', 'Cisco ASA',\n",
    "    \"Qualys\",\n",
    "    \"Splunk\",\n",
    "    \"DataStage\",\n",
    "    \"spotfire\",\n",
    "    \"sap\",\n",
    "    \"Git\",\n",
    "    \"Office Suite\",\n",
    "    \"Trello\",\n",
    "    \"Asana\",\n",
    "    \"Jira\",\n",
    "    \"Confluence\",\n",
    "    \"Zoom\",\n",
    "    \"Adobe Creative Cloud\",\n",
    "    \"Figma\",\n",
    "    \"Canva\",\n",
    "    \"Miro\",\n",
    "    \"Power BI\",\n",
    "    \"Notion\",\n",
    "    \"Monday.com\",\n",
    "    \"Dropbox\",\n",
    "    \"Box\",\n",
    "    \"SharePoint\",\n",
    "    \"Evernote\",\n",
    "    \"GitHub\",\n",
    "    \"HubSpot\",\n",
    "    \"Mailchimp\",\n",
    "    \"Zapier\",\n",
    "    \"QuickBooks\",\n",
    "    \"Toggl\",\n",
    "    \"LastPass\",\n",
    "    \"Tableau\",\n",
    "    \"Google Analytics\",\n",
    "    \"SEMrush\",\n",
    "    \"JotForm\",\n",
    "    \"Basecamp\",\n",
    "    \"ClickUp\",\n",
    "    \"InVision\",\n",
    "    \"Lucidchart\",\n",
    "    \"Visio\",\n",
    "    \"PowerPoint\",\n",
    "    \"Google Slides\",\n",
    "    \"Balsamiq\",\n",
    "    \"MindMeister\",\n",
    "    \"XMind\",\n",
    "    \"Calendly\",\n",
    "    \"RescueTime\",\n",
    "    \"Adobe Acrobat\",\n",
    "    \"Mendeley\",\n",
    "    \"EndNote\",\n",
    "    \"Excel\",\n",
    "    \"Adobe Analytics\",\n",
    "    \"Looker\",\n",
    "    \"Adobe Creative Suite\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_range(i,word_array):\n",
    "  try:\n",
    "    for j in range (i-5,i):\n",
    "        if word_array[j]== \"year\" or word_array[j]==\"years\":\n",
    "          return j\n",
    "  except:\n",
    "    return -1\n",
    "\n",
    "  return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_experience(text):\n",
    "  word_array = text.split()\n",
    "  for i in range(len(word_array)) :\n",
    "    if word_array[i] == \"experience\" or word_array[i]==\"experiences\":\n",
    "      j=in_range(i,word_array)\n",
    "      if j != -1:\n",
    "        return word_array[j-1]\n",
    "  return \"Not_specified\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skills(text,skillset):\n",
    "    skills = []\n",
    "    for i in skillset :\n",
    "      if i.lower() in text.lower():\n",
    "        skills.append(i)\n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = \"C:\\\\Users\\\\ahmad\\\\Downloads\\\\chromedriver-win64\\\\chromedriver-21nov24\\\\chromedriver.exe\"\n",
    "service = Service(executable_path=driver_path)\n",
    "wd = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days=3\n",
    "\n",
    "base_url_arr=[\"https://malaysia.indeed.com/q-data-analyst-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-data-scientist-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-data-engineer-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-full-stack-developer-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-front-end-developer-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-back-end-developer-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-business-analyst-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-cybersecurity-jobs.html\",\n",
    "              \"https://malaysia.indeed.com/q-cloud-engineer-jobs.html\"\n",
    "              ]\n",
    "\n",
    "base_url_no_html_arr=[f\"https://malaysia.indeed.com/jobs?q=data+analyst&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=data+scientist&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=data+engineer&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=full+stack+developer&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=front+end+developer&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=back+end+developer&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=business+analyst&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=cybersecurity&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=cloud+engineer&fromage={days}\",\n",
    "                      f\"https://malaysia.indeed.com/jobs?q=software+engineer&fromage={days}\"\n",
    "                      ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "job_dataframes=[]\n",
    "for a in range(len(base_url_no_html_arr)) :\n",
    "\n",
    "    payload = { 'api_key': '', 'url': base_url_no_html_arr[a],'render': 'false' }\n",
    "    r = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "\n",
    "    with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"w\") as file:\n",
    "            file.write(r.text)\n",
    "\n",
    "    with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    job_count_span = soup.find(\"div\", class_=\"jobsearch-JobCountAndSortPane-jobCount\")\n",
    "\n",
    "    # Check if the span was found and print it\n",
    "    if job_count_span:\n",
    "        print(job_count_span.text)\n",
    "    else:\n",
    "        print(\"Span not found.\")\n",
    "\n",
    "    job_count_text = job_count_span.text \n",
    "    job_count = re.sub(r'\\D', '', job_count_text)\n",
    "    job_count=int(job_count)\n",
    "\n",
    "    # Initialize a list to store each job's data\n",
    "\n",
    "    dataframes=[]\n",
    "    number_of_pages=int(job_count/14)\n",
    "    if(number_of_pages==0):\n",
    "        number_of_pages=1 \n",
    "    print(\"Number of pages : \",number_of_pages)\n",
    "    for i in range(number_of_pages):\n",
    "        payload = { 'api_key': 'a0b77ca24b816463a088b4cda133ce04', 'url': base_url_no_html_arr[a]+f\"&start={i*10}\",'render':'false' }\n",
    "        r = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "        #wd.get(base_url_no_html_arr[a]+f\"&start={i*10}\")\n",
    "        \n",
    "        with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"w\") as file:\n",
    "            file.write(r.text)\n",
    "        with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "            \n",
    "        jobs_data = []\n",
    "        # Find all job entries based on the class containing job links\n",
    "        for job in soup.find_all(\"a\", class_=\"jcs-JobTitle\"):\n",
    "            # Extract the job ID\n",
    "            job_id = job.get(\"id\")\n",
    "            \n",
    "            # Navigate to the main container of each job posting and extract relevant data\n",
    "            job_container = job.find_parent(\"td\")\n",
    "            \n",
    "            # Get job title\n",
    "            job_title = job.text.strip()\n",
    "            \n",
    "            # Extract company name\n",
    "            company_name = job_container.find(\"span\", {\"data-testid\": \"company-name\"}).text.strip()\n",
    "            \n",
    "            # Extract location\n",
    "            location = job_container.find(\"div\", {\"data-testid\": \"text-location\"}).text.strip()\n",
    "            \n",
    "            # Extract salary using the specified class name\n",
    "            salary_div = job_container.find(\"div\", class_=\"salary-snippet-container\")\n",
    "            salary = salary_div.find(\"div\", class_=\"css-18z4q2i eu4oa1w0\").text.strip() if salary_div else None\n",
    "            \n",
    "            # Extract job type using the specified class name\n",
    "            job_type_div = job_container.find(\"li\", class_=\"metadata css-1f4kgma eu4oa1w0\")\n",
    "            job_type = job_type_div.find(\"div\", class_=\"css-18z4q2i eu4oa1w0\").text.strip() if job_type_div else None\n",
    "            \n",
    "            # Extract schedule using the specified class name\n",
    "            schedule_div = job_container.find_all(\"li\", class_=\"metadata css-1f4kgma eu4oa1w0\")\n",
    "            schedule = schedule_div[1].find(\"div\", class_=\"css-18z4q2i eu4oa1w0\").text.strip() if len(schedule_div) > 1 else None\n",
    "\n",
    "            pages = i+1\n",
    "            \n",
    "            # Store all extracted data in a dictionary\n",
    "            job_info = {\n",
    "                \"Job ID\": job_id,\n",
    "                \"Job Title\": job_title,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Salary\": salary,\n",
    "                \"Job Type\": job_type,\n",
    "                \"Schedule\": schedule,\n",
    "                \"Pages\":pages\n",
    "            }\n",
    "            \n",
    "            # Append job info to the list\n",
    "            jobs_data.append(job_info)\n",
    "\n",
    "        # Create a DataFrame from the jobs_data list\n",
    "        jobs_df = pd.DataFrame(jobs_data)\n",
    "\n",
    "        \n",
    "        job_requirements = []\n",
    "        year_of_experiences=[]\n",
    "        skills_col=[]\n",
    "        for j in range(len(jobs_df)):\n",
    "            try:\n",
    "                payload = { 'api_key': 'a0b77ca24b816463a088b4cda133ce04', 'url': base_url_no_html_arr[a]+f\"&start={i*10}&vjk={jobs_df.iloc[j][\"Job ID\"].split(\"_\")[1]}\",'render':'false' }\n",
    "                r = requests.get('https://api.scraperapi.com/', params=payload)\n",
    "                #wd.get(base_url_no_html_arr[a]+f\"&start={i*10}&vjk={jobs_df.iloc[j][\"Job ID\"].split(\"_\")[1]}\")\n",
    "                #time.sleep(5)\n",
    "\n",
    "                with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\subpages\\\\subpage_indeed.html\", \"w\") as file:\n",
    "                        file.write(r.text)\n",
    "\n",
    "                with open('C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\subpages\\\\subpage_indeed.html', 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "                description_div = soup.find(\"div\", id=\"jobDescriptionText\")\n",
    "                text_array=[]\n",
    "                if description_div:\n",
    "                    # Extract all <li> tags\n",
    "                    for li in description_div.find_all(\"li\"):\n",
    "                        text_array.append(li.get_text(strip=True))\n",
    "\n",
    "                    # Extract all <p> tags\n",
    "                    for p in description_div.find_all(\"p\"):\n",
    "                        text_array.append(p.get_text(strip=True))\n",
    "\n",
    "                concatenated_string = \" \".join(text_array)\n",
    "                #job_requirements.append(concatenated_string)\n",
    "                skills = programming_languages + frameworks + databases + cloud_service_providers + other_tools\n",
    "                skills = list(set(skills))\n",
    "        \n",
    "                for k in range (len(skills)) :\n",
    "                    if len(skills[k])<=10:\n",
    "                        skills[k]=\" \"+skills[k]+\" \"\n",
    "\n",
    "                concatenated_string = \" \"+concatenated_string+\" \"\n",
    "                concatenated_string = re.sub(r'(?<![A-Za-z0-9])\\.|\\.(?![A-Za-z0-9])|[^A-Za-z0-9.#+]', ' ', concatenated_string)\n",
    "                #print(text_array)\n",
    "                #print(concatenated_string)\n",
    "\n",
    "\n",
    "\n",
    "                skills_needed=extract_skills(concatenated_string,skills)\n",
    "                experience=extract_experience(concatenated_string)\n",
    "\n",
    "                job_requirements.append(concatenated_string)\n",
    "                year_of_experiences.append(experience)\n",
    "                skills_col.append(skills_needed)\n",
    "                \n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"An error occured\")\n",
    "                job_requirements.append(\"error\")\n",
    "                year_of_experiences.append(\"error\")\n",
    "                skills_col.append([])\n",
    "                wd.quit()\n",
    "                wd = webdriver.Chrome(service=service)\n",
    "\n",
    "\n",
    "        jobs_df[\"Job Requirements\"]=job_requirements\n",
    "        jobs_df[\"Skills\"]=skills_col\n",
    "        jobs_df[\"Year Of Experience\"]=year_of_experiences\n",
    "\n",
    "        dataframes.append(jobs_df)\n",
    "        print(\"Page \",i+1,\" has been scraped\")\n",
    "\n",
    "    df = pd.concat(dataframes)\n",
    "    job_dataframes.append(df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "job_dataframes=[]\n",
    "for a in range(len(base_url_no_html_arr)) :\n",
    "\n",
    "    time.sleep(5)\n",
    "    wd.get(base_url_no_html_arr[a])\n",
    "    with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"w\") as file:\n",
    "            file.write(wd.page_source)\n",
    "\n",
    "    with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    job_count_span = soup.find(\"div\", class_=\"jobsearch-JobCountAndSortPane-jobCount\")\n",
    "\n",
    "    # Check if the span was found and print it\n",
    "    '''\n",
    "    if job_count_span:\n",
    "        print(job_count_span.text)\n",
    "    else:\n",
    "        print(\"Span not found.\")\n",
    "        wd.quit()\n",
    "        wd = webdriver.Chrome(service=service)\n",
    "        wd.get(base_url_no_html_arr[a])\n",
    "        with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"w\") as file:\n",
    "                file.write(wd.page_source)\n",
    "\n",
    "        with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        job_count_span = soup.find(\"div\", class_=\"jobsearch-JobCountAndSortPane-jobCount\")\n",
    "    '''\n",
    "\n",
    "    while not job_count_span:\n",
    "        print(\"Span not found.\")\n",
    "        wd.quit()\n",
    "        wd = webdriver.Chrome(service=service)\n",
    "        wd.get(base_url_no_html_arr[a])\n",
    "        with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"w\") as file:\n",
    "                file.write(wd.page_source)\n",
    "\n",
    "        with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        job_count_span = soup.find(\"div\", class_=\"jobsearch-JobCountAndSortPane-jobCount\")\n",
    "        \n",
    "    print(job_count_span.text)\n",
    "    job_count_text = job_count_span.text \n",
    "    job_count = re.sub(r'\\D', '', job_count_text)\n",
    "    job_count=int(job_count)\n",
    "\n",
    "    # Initialize a list to store each job's data\n",
    "\n",
    "    dataframes=[]\n",
    "    number_of_pages=int(job_count/14)\n",
    "    if(number_of_pages==0):\n",
    "        number_of_pages=1 \n",
    "    print(\"Number of pages : \",number_of_pages)\n",
    "    for i in range(number_of_pages):\n",
    "        wd.quit()\n",
    "        wd = webdriver.Chrome(service=service)\n",
    "        wd.get(base_url_no_html_arr[a]+f\"&start={i*10}\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"w\") as file:\n",
    "            file.write(wd.page_source)\n",
    "        with open(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\test_indeed.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "            content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "            \n",
    "        jobs_data = []\n",
    "        # Find all job entries based on the class containing job links\n",
    "        for job in soup.find_all(\"a\", class_=\"jcs-JobTitle\"):\n",
    "            # Extract the job ID\n",
    "            job_id = job.get(\"id\")\n",
    "            \n",
    "            # Navigate to the main container of each job posting and extract relevant data\n",
    "            job_container = job.find_parent(\"td\")\n",
    "            \n",
    "            # Get job title\n",
    "            job_title = job.text.strip()\n",
    "            \n",
    "            # Extract company name\n",
    "            company_name = job_container.find(\"span\", {\"data-testid\": \"company-name\"}).text.strip()\n",
    "            \n",
    "            # Extract location\n",
    "            location = job_container.find(\"div\", {\"data-testid\": \"text-location\"}).text.strip()\n",
    "            \n",
    "            # Extract salary using the specified class name\n",
    "            salary_div = job_container.find(\"div\", class_=\"salary-snippet-container\")\n",
    "            salary = salary_div.find(\"div\", class_=\"css-18z4q2i eu4oa1w0\").text.strip() if salary_div else None\n",
    "            \n",
    "            # Extract job type using the specified class name\n",
    "            job_type_div = job_container.find(\"li\", class_=\"metadata css-1f4kgma eu4oa1w0\")\n",
    "            job_type = job_type_div.find(\"div\", class_=\"css-18z4q2i eu4oa1w0\").text.strip() if job_type_div else None\n",
    "            \n",
    "            # Extract schedule using the specified class name\n",
    "            schedule_div = job_container.find_all(\"li\", class_=\"metadata css-1f4kgma eu4oa1w0\")\n",
    "            schedule = schedule_div[1].find(\"div\", class_=\"css-18z4q2i eu4oa1w0\").text.strip() if len(schedule_div) > 1 else None\n",
    "\n",
    "            pages = i+1\n",
    "            \n",
    "            # Store all extracted data in a dictionary\n",
    "            job_info = {\n",
    "                \"Job ID\": job_id,\n",
    "                \"Job Title\": job_title,\n",
    "                \"Company Name\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Salary\": salary,\n",
    "                \"Job Type\": job_type,\n",
    "                \"Schedule\": schedule,\n",
    "                \"Pages\":pages\n",
    "            }\n",
    "            \n",
    "            # Append job info to the list\n",
    "            jobs_data.append(job_info)\n",
    "\n",
    "        # Create a DataFrame from the jobs_data list\n",
    "        jobs_df = pd.DataFrame(jobs_data)\n",
    "\n",
    "        \n",
    "        job_requirements = []\n",
    "        year_of_experiences=[]\n",
    "        skills_col=[]\n",
    "        for j in range(len(jobs_df)):\n",
    "            try:\n",
    "                wd.quit()\n",
    "                wd = webdriver.Chrome(service=service)\n",
    "                wd.get(base_url_no_html_arr[a]+f\"&start={i*10}&vjk={jobs_df.iloc[j][\"Job ID\"].split(\"_\")[1]}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "                with open(f\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\subpages\\\\subpage_indeed.html\", \"w\") as file:\n",
    "                        file.write(wd.page_source)\n",
    "\n",
    "                with open('C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\html_scripts\\\\subpages\\\\subpage_indeed.html', 'r', encoding='utf-8') as file:\n",
    "                    soup = BeautifulSoup(file, 'html.parser')\n",
    "\n",
    "                description_div = soup.find(\"div\", id=\"jobDescriptionText\")\n",
    "                text_array=[]\n",
    "                if description_div:\n",
    "                    # Extract all <li> tags\n",
    "                    for li in description_div.find_all(\"li\"):\n",
    "                        text_array.append(li.get_text(strip=True))\n",
    "\n",
    "                    # Extract all <p> tags\n",
    "                    for p in description_div.find_all(\"p\"):\n",
    "                        text_array.append(p.get_text(strip=True))\n",
    "\n",
    "                concatenated_string = \" \".join(text_array)\n",
    "                #job_requirements.append(concatenated_string)\n",
    "                skills = programming_languages + frameworks + databases + cloud_service_providers + other_tools\n",
    "                skills = list(set(skills))\n",
    "        \n",
    "                for k in range (len(skills)) :\n",
    "                    if len(skills[k])<=10:\n",
    "                        skills[k]=\" \"+skills[k]+\" \"\n",
    "\n",
    "                concatenated_string = \" \"+concatenated_string+\" \"\n",
    "                concatenated_string = re.sub(r'(?<![A-Za-z0-9])\\.|\\.(?![A-Za-z0-9])|[^A-Za-z0-9.#+]', ' ', concatenated_string)\n",
    "                #print(text_array)\n",
    "                #print(concatenated_string)\n",
    "\n",
    "\n",
    "\n",
    "                skills_needed=extract_skills(concatenated_string,skills)\n",
    "                experience=extract_experience(concatenated_string)\n",
    "\n",
    "                job_requirements.append(concatenated_string)\n",
    "                year_of_experiences.append(experience)\n",
    "                skills_col.append(skills_needed)\n",
    "                \n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"An error occured\")\n",
    "                job_requirements.append(\"error\")\n",
    "                year_of_experiences.append(\"error\")\n",
    "                skills_col.append([])\n",
    "                wd.quit()\n",
    "                wd = webdriver.Chrome(service=service)\n",
    "\n",
    "\n",
    "        jobs_df[\"Job Requirements\"]=job_requirements\n",
    "        jobs_df[\"Skills\"]=skills_col\n",
    "        jobs_df[\"Year Of Experience\"]=year_of_experiences\n",
    "\n",
    "        dataframes.append(jobs_df)\n",
    "        print(\"Page \",i+1,\" has been scraped\")\n",
    "\n",
    "    df = pd.concat(dataframes)\n",
    "    job_dataframes.append(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataframes)\n",
    "wd.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.concat(job_dataframes)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def det_internship(row):\n",
    "    if (\"intern\" in row[\"Job Title\"].lower()) :\n",
    "        return \"Internship\"\n",
    "    else :\n",
    "        return row[\"Year Of Experience\"]\n",
    "\n",
    "df_1 = final_df.copy()\n",
    "\n",
    "df_1[\"Experience_new\"]= df_1.apply( det_internship, axis=1)\n",
    "df_1.drop(columns=[\"Year Of Experience\"],inplace=True)\n",
    "df_1.rename(columns={\"Experience_new\":\"Year Of Experience\"},inplace=True)\n",
    "df_1.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\indeed_test1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_dup(job_id):\n",
    "    occurences = 0\n",
    "    for i in df_1[\"Job ID\"]:\n",
    "        if i==job_id:\n",
    "            occurences = occurences+1\n",
    "    if(occurences>1):\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "condition_arr=[]\n",
    "for i in range(len(df_1)):\n",
    "    if has_dup(df_1.iloc[i][\"Job ID\"]) and len(df_1.iloc[i][\"Job Requirements\"])<6 :\n",
    "        condition_arr.append(True)\n",
    "    else:\n",
    "        condition_arr.append(False)\n",
    "\n",
    "df_1[\"Condition\"]=condition_arr\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_1.copy()\n",
    "df_2 = df_2[df_2[\"Condition\"]==False]\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3=df_2.copy()\n",
    "df_3 = df_3.drop_duplicates(subset='Job ID')\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_job=[]\n",
    "for index,rows in df_3.iterrows():\n",
    "    if \"data\" in rows[\"Job Title\"].lower() and \"scien\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Data Scientist\")\n",
    "    elif \"business\" in rows[\"Job Title\"].lower() and (\"anal\" in rows[\"Job Title\"].lower() or \"intell\" in rows[\"Job Title\"].lower()):\n",
    "        short_job.append(\"Business Analyst\")\n",
    "    elif \"data\" in rows[\"Job Title\"].lower() and \"engine\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Data Engineer\")\n",
    "    elif \"data\" in rows[\"Job Title\"].lower() and \"anal\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Data Analyst\")\n",
    "    elif \"machine\" in rows[\"Job Title\"].lower() and \"learn\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Data Scientist\")\n",
    "    elif \"full\" in rows[\"Job Title\"].lower() and \"stack\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append('Full Stack Developer')\n",
    "    elif \"back\" in rows[\"Job Title\"].lower() and \"end\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Backend Developer\")\n",
    "    elif \"front\" in rows[\"Job Title\"].lower() and \"end\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Frontend Developer\")\n",
    "    elif \"software\" in rows[\"Job Title\"].lower() and (\"engine\" in rows[\"Job Title\"].lower() or \"develop\" in rows[\"Job Title\"].lower()):\n",
    "        short_job.append(\"Developer\")\n",
    "    elif \"developer\" in rows[\"Job Title\"].lower() or \"programmer\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Developer\")\n",
    "    elif \"cloud\" in rows[\"Job Title\"].lower() or \"devop\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Cloud Engineer\")\n",
    "    elif \"secu\" in rows[\"Job Title\"].lower() or \" soc \" in rows[\"Job Title\"].lower() or \"threat\" in rows[\"Job Title\"].lower():\n",
    "        short_job.append(\"Cybersecurity\")\n",
    "    else :\n",
    "        short_job.append(\"Not identified\")\n",
    "\n",
    "\n",
    "df_3[\"Short Job Title\"]=short_job\n",
    "df_3=df_3[df_3[\"Short Job Title\"]!=\"Not identified\"]\n",
    "df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.to_csv(\"C:\\\\Users\\\\ahmad\\\\OneDrive\\\\文档\\\\WebScraping101\\\\3day_indeed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_count = df_3.groupby(\"Short Job Title\")[\"Short Job Title\"].count()\n",
    "print(job_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format the output\n",
    "formatted_date = now.strftime(\"%Y-%m-%d\")\n",
    "formatted_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "print(f\"Latest scraping date: {formatted_date}\")\n",
    "print(f\"Current time: {formatted_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
